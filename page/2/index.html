<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"qiubite31.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="Chase excellence, success will follow!">
<meta property="og:type" content="website">
<meta property="og:title" content="Drake&#39;s">
<meta property="og:url" content="https://qiubite31.github.io/page/2/index.html">
<meta property="og:site_name" content="Drake&#39;s">
<meta property="og:description" content="Chase excellence, success will follow!">
<meta property="og:locale" content="zh_TW">
<meta property="article:author" content="Drake">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://qiubite31.github.io/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-TW'
  };
</script>

  <title>Drake's</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-82135937-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-82135937-1');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切換導航欄">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Drake's</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首頁</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>標籤</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分類</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>歸檔</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜尋
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜尋..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2020/02/14/Machine-Learning-Techniques-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/14/Machine-Learning-Techniques-5/" class="post-title-link" itemprop="url">機器學習技法(Machine Learning Techniques)第五講筆記</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-02-14 00:00:00" itemprop="dateCreated datePublished" datetime="2020-02-14T00:00:00+08:00">2020-02-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:45:21" itemprop="dateModified" datetime="2021-05-11T23:45:21+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2020/02/14/Machine-Learning-Techniques-5/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/02/14/Machine-Learning-Techniques-5/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <img src="/2020/02/14/Machine-Learning-Techniques-5/wrapup.JPG" class="" width="500">

<p>上一堂講到在允許違反部份邊界下，引入C當作懲罰值，將Hard-Margin SVM轉成Soft-Margin。而Soft-Margin的對偶問題和Hard-Margin非常相似，只差在對偶問題中的α有上限值C。這堂課會談如果將kernel trick引入logistic regression可以怎麼作。</p>
<img src="/2020/02/14/Machine-Learning-Techniques-5/SlackVariable.JPG" class="" width="500">

<p>回顧Soft-Margin SVM，裡面發生的違反邊界會被紀錄在ξn裡面，而ξn會是1-yn(W．Zn+b)，因為會紀錄下與1的距離來表式違反的程度和嚴重性；相反的在沒有違反下ξn值為0。再來可以把ξn寫成另一個更簡單的式子，即為轉換成對1-yn(W．Zn+b)和0之間取最大值來算出ξn，並帶入SVM的最佳化式子中，將ξn轉成b和w算出來的結果。</p>
<img src="/2020/02/14/Machine-Learning-Techniques-5/UnconstrainForm.JPG" class="" width="500">

<p>這個轉換後的最佳式其實和之前教過的regularization非常相似，就是在求長度w控制複雜度之下加上regularization項次。所以從這裡可以看到，Soft-Margin SVM其實可以從regularization推導過來，但上一講會選擇從Hard-Margin推導過來的原因是因為這個式子並不是一個QP問題，而且在兩項取最大值時也會有微分求解問題。</p>
<img src="/2020/02/14/Machine-Learning-Techniques-5/SVMasRegularization.JPG" class="" width="500">

<p>在前面課程推導對偶問題時有提到SVM和regularization的相似地方，而這裡可以看到Soft-Margin SVM其實就和L2 regularization是相似的，所以SVM的large margin就是一個對regularization的實現。但其中細部的差異在於Soft-Margin會使用到特別的error表示，而C的大小即為控制regularization的程度(越大的C代表越小的regularization)。</p>
<img src="/2020/02/14/Machine-Learning-Techniques-5/AlgorithmErrorMeasure.JPG" class="" width="500">

<p>在計算zero-one error時會是一個像階梯狀的函數，因為ys是正的error為0(猜的方向正確)，ys如果是負的error為1(猜的方向錯誤)。而SVM的error會由兩個線段組成，在ys離1越遠error會越大，而ys為正時error為0，SVM的error為zero-one error的上限，所以Soft-Margin也是在間接的把zeor-one error作好。</p>
<img src="/2020/02/14/Machine-Learning-Techniques-5/SVMandLogisticRegression.JPG" class="" width="500">

<p>如果將SVM和logistic regression比較的話，從圖上可以發現logistic regression會和SVM很接近，進一步將ys在正無限大和負無限大比較時，兩者的error也是相近的，所以SVM其實也很像在作L2-regularized，因為在加上error項後，其實error項和logistic regression的error項非常接近。</p>
<img src="/2020/02/14/Machine-Learning-Techniques-5/LinearModelClassification.JPG" class="" width="500">

<p>在比較三種兩元分類方法，最一開始教的PLA需要在線性可分的情況下才好作，如果在線性不可分要透過pocket也不好達成；logistic regression的函數有很好的最佳化性質，而且加上L2-regularized後還能對模型的複雜度有一定的保護；Soft-Margin SVM差異在於最佳化使用了QP，並且在有large margin的理論保證，其error的計算方式雖然和logistic regression不同，但確實非常接近，而且兩者都是在作最佳化zero-one error的上限值。因此regularized logistic regression其實就幾乎是在作Soft-Margin SVM。</p>
<img src="/2020/02/14/Machine-Learning-Techniques-5/BinaryClassfication.JPG" class="" width="500">

<p>在二元分類問題中可以有兩種簡易的方法，第一種是透過SVM找到最佳的b和w，再直接丟進logistic reression作出二元分類，但這樣的手法就缺少了點logistic regression像是maximum likelihood的特色；第二種則是將SVM找到的最佳b和w，放進logistic regression當作始初解，之後再作最佳化找到最佳解，但這樣和本來logistic regression用其他初始解再找最佳解的結果相同，而且解變動後也失去了SVM的特性。</p>
<img src="/2020/02/14/Machine-Learning-Techniques-5/TwoLevelLearning.JPG" class="" width="500">

<p>如果要同時有兩種方法的特性，可以先作SVM找到分數，再加上放縮的A與平移的B，最後再放進logistic regression訓練，其意義為先透過SVM找到最佳的超平面，再透過logistic regression作放縮與平移調整到最佳。也就是先透過SVM當作轉換，再丟進logistic regression作兩階段的的學習。</p>
<img src="/2020/02/14/Machine-Learning-Techniques-5/ProbabilisticSVM.JPG" class="" width="500">

<p>透過這樣的方式就可得到SVM的Soft binary clasifier，因為引入縮放和平移所以會和原本的SVM會不太相同，在求解logistic regression時可以使用Gradient Descent即可。到這裡我們透過kernel SVM去推論logistic regression在z空間的近似解，但這裡不是真的在z空間解出logistic regression，而且透過SVM在z空間找近似解。</p>
<img src="/2020/02/14/Machine-Learning-Techniques-5/KeyBehindKernelTrick.JPG" class="" width="500">

<p>複習一下SVM能使用kernel的關鍵點，是在於將W．Z的內積問題，將W轉換成一堆Z的線性組合，成為求Z．Z內積問題，這時候就能讓kernel上場。在SVM中，W就是Zn的線性組合，組合的系數就是對偶問題的解；PLA的W也是Zn的線性組合，組合的系數取決定每次犯錯的修正，而logistic regression中的W也是Zn的線性組合，這些系數來自梯度下降最終求得的結果。所以這些方法應該都能夠應用kernel trick輕易的在取得z空間的解，最好的W即可透過這些Zn來表達出來。</p>
<img src="/2020/02/14/Machine-Learning-Techniques-5/RepresenterTheorem.JPG" class="" width="500">

<p>老師在這裡用例子證明任何的L2-regularized線性模型皆可以應用kernel。</p>
<img src="/2020/02/14/Machine-Learning-Techniques-5/KernelLogisticRegression.JPG" class="" width="500">

<p>因此，在L2-regularized的logistic regression，就可以將W替換成Zn的線性組合，從求W轉成求所有β系數。在將Zn線數組合代進入取代W後，就能將Z．Z的部份使用kernel，接著就可以透過梯度下降來求最佳解，這就是kernel logistic regression。</p>
<img src="/2020/02/14/Machine-Learning-Techniques-5/KLROtherView.JPG" class="" width="500">

<p>如果看這個kernel logistic regreaaion，先從β的角度看起來就像是在求β的線性組合，而且其中還將kernel用來作轉換與正規化；如果是從w角度來看，即為藏有kernel轉換與L2正規化的線性模型。但要注意常常β解出來會有很多0。</p>
<img src="/2020/02/14/Machine-Learning-Techniques-5/Summary.JPG" class="" width="500">

<p>總結這堂課從將Soft-Margin SVM解釋成使用hinge error的regularize model，再進一步和L2-regularized logistic regresion比較兩者的相似性，透過結合兩者建立two-level learning應用在Soft Binary Classification，最後真的證明logistic regression也可以應用kernel並求出z空間的解，但會付出解出來很多β會為0的代價。</p>
<p>參考資料:<br><a target="_blank" rel="noopener" href="https://www.csie.ntu.edu.tw/~htlin/mooc/doc/205_handout.pdf">Machine Learning Techniques 5</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2020/02/05/Machine-Learning-Techniques-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/05/Machine-Learning-Techniques-4/" class="post-title-link" itemprop="url">機器學習技法(Machine Learning Techniques)第四講筆記</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-02-05 00:00:00" itemprop="dateCreated datePublished" datetime="2020-02-05T00:00:00+08:00">2020-02-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:45:13" itemprop="dateModified" datetime="2021-05-11T23:45:13+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2020/02/05/Machine-Learning-Techniques-4/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/02/05/Machine-Learning-Techniques-4/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <img src="/2020/02/05/Machine-Learning-Techniques-4/HardMarginCons.JPG" class="" width="500">

<p>上一講教了透過kernel trick來處理dual SVM中的轉換與內積，並介紹了不同的kernel function，從線性到無限多維轉換的Gaussian，但SVM還是有可能會overfit。overfit有兩種可能的原因，第一種為使用了過度複雜的轉換，第二種則是堅持要把所有資料完全的切分開來。以上面這個例子來說，如果可以容忍有錯誤分類，但可以使用簡單的線性切分也可以得到不錯的邊界；相反的，如果使用比較複雜的轉換確實完美的將資料都區分正確，反而容易造成overfit。</p>
<img src="/2020/02/05/Machine-Learning-Techniques-4/GiveUpExample.JPG" class="" width="500">

<p>但該如何解決這個問題呢？之前在講pocket演算法時，pocket會找到能分錯資料最少的超平面，但hard-margin SVM除了要求要全部分對之外，還要選w長度最小的超平面。所以如果可以讓SVM也能像pocket一樣容忍一些錯誤，即不管部份分錯的點，就能達到某種程度的條件放寬。這邊引入C參數來代表放寬條件的相對重要性，C較大時代表越重視不要犯錯越好，C越小時代表越重視w長度，即分對的資料的margin越寬越好。</p>
<img src="/2020/02/05/Machine-Learning-Techniques-4/SoftMargin1.JPG" class="" width="500">

<p>但放寬後的新問題將會違反QP的條件(min為二次式，條件要為一次式)，而且在允許犯錯的情況下，沒有辦法分辦到底犯錯的嚴重程度(小錯或是大錯)。為了解決這兩個問題，引入ξn來紀錄離想要的值(是否靠近1)有多近，即將犯了幾個錯誤轉成犯了多大的錯誤。除了可以解決無法分辨犯錯程度的問題，也可以讓問題符合QP。</p>
<img src="/2020/02/05/Machine-Learning-Techniques-4/SoftMargin2.JPG" class="" width="500">

<p>如同前面說的，C可以用來控制large margin和margin violation，越大的C代表越重視分類的正確性，而越小的C代表margin可以比較大，但會造成某些資料分錯。</p>
<img src="/2020/02/05/Machine-Learning-Techniques-4/SimplifyBeta.JPG" class="" width="500">

<p>當我們將Hard-Margin轉成Soft-Margin後，再來就是將它轉成對偶問題，就可以如同Hard-Margin一樣引入不同不同維度轉換的kernel function。第一步就是帶入Lagrange，有別於原本的Hard-Margin只需要帶入一個α，因為這邊分成兩個部分，因此分別帶入α和β。再進一步將ξn作微分拿掉ξ，可以發現在最佳解的位置C會等於αn+βn，就可以將βn簡化成C-αn，又因前後兩項分別都有C-αn互相消掉，所以可以再將式子簡化。</p>
<img src="/2020/02/05/Machine-Learning-Techniques-4/OtherSimplify.JPG" class="" width="500">

<p>到這邊可以發現內部要解的問題其實就和之前的Hard-Margin SVM相同，只差在外面的條件帶入了C的限制，所以這邊可以和之前一樣分別對b和w作微分來得到Soft-Margin SVM的對偶問題。</p>
<img src="/2020/02/05/Machine-Learning-Techniques-4/SoftMarginSVMDual.JPG" class="" width="500">

<p>再仔細看一下Soft-Margin SVM的對偶問題，可以發現和Hard-Margin SVM的對偶問題幾乎一樣，只差在現在α會有一個上限值C，而這個C是來自引入β後得到的。</p>
<img src="/2020/02/05/Machine-Learning-Techniques-4/SoftMarginGaussianSVM.JPG" class="" width="500">

<p>當我們把Gaussian SVM用在Soft-Margin上並調整不同的C值時，可以發現最左邊當C設成1的時候，會得到一個邊界但有部份的資料是分錯的；當C一直調到到100可以發現邊界會越來越複雜，但是分錯的資料會越來越少。其中最右邊的邊界雖然可以讓所有的資料分對，但也可以造成雜訊的容忍度下降，也就是容易造成overfit，因此C和γ值需要慎選。</p>
<img src="/2020/02/05/Machine-Learning-Techniques-4/SelectionbyCrossValidation.JPG" class="" width="500">

<p>介紹完Soft-Margin和kernel，參數該怎麼選呢?第一個方法可以嘗試使用cross validation的方式，並帶入不同的C和γ值來找到比較好的參數組合。</p>
<img src="/2020/02/05/Machine-Learning-Techniques-4/LeaveOneOutError.JPG" class="" width="500">

<p>另一個有趣的方法則是使用Leave-One-Out CV，SVM在使用Leave-One-Out CV的error會小於等於support vector的比例(support vector的數量除上所有樣本數)，其概念為即使少了non support vector(non-SV)的點，結果並不會影響到margin的計算。</p>
<img src="/2020/02/05/Machine-Learning-Techniques-4/SelectionbySV.JPG" class="" width="500">

<p>所以也可以嘗使用support vector的數量來選擇模型，因此在作cross validation前可以透過檢查support vector的數量，去掉support vector數量較多的組合作模型的安全檢查。</p>
<img src="/2020/02/05/Machine-Learning-Techniques-4/summary.JPG" class="" width="500">

<p>總結這堂課介紹了soft-margin SVM，其核心概念在於不強求把所有的類別都分對，並加上犯錯的程度大小的懲罰項。在推導的部份，Soft-Margin和Hard-Margin結果幾乎一樣，只差在Soft-Margin在αn會存在上限值C。而SVM可以將資料分成三種，分別為non-SV、存在邊界上的free-SV與可能違反邊界的SV。最後在model selection可以使用cross-validation或是參數SV的數量來作選擇。</p>
<p>參考資料:<br><a target="_blank" rel="noopener" href="https://www.csie.ntu.edu.tw/~htlin/mooc/doc/204_handout.pdf">Machine Learning Techniques 4</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2020/01/30/Machine-Learning-Techniques-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/30/Machine-Learning-Techniques-3/" class="post-title-link" itemprop="url">機器學習技法(Machine Learning Techniques)第三講筆記</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-01-30 00:00:00" itemprop="dateCreated datePublished" datetime="2020-01-30T00:00:00+08:00">2020-01-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:45:05" itemprop="dateModified" datetime="2021-05-11T23:45:05+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2020/01/30/Machine-Learning-Techniques-3/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/01/30/Machine-Learning-Techniques-3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <img src="/2020/01/30/Machine-Learning-Techniques-3/DualSVMRevisit.JPG" class="" width="500">

<p>上一講說明了dual SVM，好像可以讓SVM與高維度的z空間兩者可以脫鉤，但以計算的角度，仔細看會發現在整個求解過程中的Q矩陣還是會在z空間作內積。而在z空間的內積在運算可以拆成兩個部份，一個是先透過某個函式φ轉換到z空間，第二再進行轉換後的內積。但這樣的運算方法就會變成z空間的維度作內積。</p>
<img src="/2020/01/30/Machine-Learning-Techniques-3/FastInnerProduct.JPG" class="" width="500">

<p>透過這個polynomial transform可以發現，其實是能先在x維度作完內積，再作函式φ的轉換，這樣就可避免產生在z空間作內積的情況，透過這個簡化的方式計算內積稱為kernel function。</p>
<img src="/2020/01/30/Machine-Learning-Techniques-3/KernelTrick.JPG" class="" width="500">

<p>回到svm不管是在計算b和最佳的超平面gsvm時，都可以將kernel function應用在計算中。這種合併函式轉換和內積計算，來避免在在高級度空間進行內積計算的方法，又稱為kernel trick。</p>
<img src="/2020/01/30/Machine-Learning-Techniques-3/GemeralPoly2Kernel.JPG" class="" width="500">

<p>延續前面的Poly-2 kernel例子，在每項前面再加上不同的係數，就能再對計算進一步簡化。雖然不同的函式都是轉換到相同的維度，但是會計算出不一樣的內積得到不同的距離，所以得到的幾合意義和算出來的SVM margin也會不同。其中最常使用的kernel為加上根號2與γ值的General Poly-2 kernel。</p>
<img src="/2020/01/30/Machine-Learning-Techniques-3/Poly2Action.JPG" class="" width="500">

<p>舉例來說，中間為原本的Poly-2 kernel，而左右兩個為General Poly-2 kernel但代入不同的γ值，左邊帶很小的γ右邊帶很大的γ。可以看到得出的邊界是不同的，所以得出的支撐向量也不同(方框的點)。雖然三個kernel的邊界不同，margin的定義也不同，但這邊沒辦法說哪個邊界會比較好，因為三個kernel都可以把點分開。</p>
<img src="/2020/01/30/Machine-Learning-Techniques-3/GeneralPolyKernel.JPG" class="" width="500">

<p>前面提到的Poly-2可以再加入ζ參數，就會形成一個包含3個參數的一般型態Polynomial Kernel，包含了(1)γ控制x內積算完後的放縮程度，和(2)ζ控制常數項與乘上轉換係數如何對應(3)Q代表poly作幾次方的轉換。而且不管作了幾次方的轉換，都是透遇kernel而不會在高維空間計算。使用高次方的轉換仍然可能會有overfit的風險，但SVM會透過找到最大margin來避免overfit，所以透過kernel可以讓SVM達到避免ovefit，又可以使用較複雜的模型，同時不需要高度的計算複雜度。SVM結合polynomial kernel又稱為Polynomial kernel。</p>
<img src="/2020/01/30/Machine-Learning-Techniques-3/LinearKernel.JPG" class="" width="500">

<p>Polynomial kernel如果設定成1次轉換，γ設成1，ζ設成0的話，就等於是原本的linear kernel SVM。老師建議大家在使用SVM時，可以從linear SVM開始，如果linear就可以解決問題，就不需要作高次方的nonlinear轉換。</p>
<img src="/2020/01/30/Machine-Learning-Techniques-3/InfiniteDimensionalTrasform.JPG" class="" width="500">

<p>既然可以透過kernel trick來達成高維度轉換的運算，那是否有可能作到無限多維轉換呢？老師這裡透過證明可以發現，即使在無限多維的情況，使用高斯函數作轉換是可以辦到的。</p>
<img src="/2020/01/30/Machine-Learning-Techniques-3/GaussianSVM.JPG" class="" width="500">

<p>所以高斯函數也是一種kernel trick的方法，讓資料映射到無限多維的空間後找到支撐向量和超平面。但其實際上的意義為他會產生以支撐向量為中心的高斯函數的線性組合，也常被稱為RBF(Radial Basis Function) kernel。</p>
<img src="/2020/01/30/Machine-Learning-Techniques-3/GuassianSVMAction.JPG" class="" width="500">

<p>到目前為止，SVM可以辦到使用kernel trick作無限多維的轉換後，再透過最大margin找到超平面，而SVM裡面的最大margin計算，可以對無限多維的轉換有一定程度的保護。但要注意SVM仍然會有overfit的問題，當γ越調越大時會使原本Gaussan裡面的σ變小Gaussian就會變尖(γ=1/2σ^2)，所以還是要小心γ參數的選用會決定SVM的表現，通常不建議使用太大的γ值。</p>
<p>前面介紹到許多的kernel，不同的kernel也有其好處和壞處</p>
<img src="/2020/01/30/Machine-Learning-Techniques-3/LinearKernelProCon.JPG" class="" width="500">

<ol>
<li>Linear Kernel<br>最簡單的kernel，就是不作任何的維度轉換，其好處就是最簡單安全。除了QP好解之外也具有可解釋性，因為是線性所以可以透過每個特徵的權重來看出特徵的重要性，應該是遇到問題時要先被拿來嘗試使用的kernel。但其壞處在於問題如果是非線性的，那只靠線性沒辦法很好的解決問題。</li>
</ol>
<img src="/2020/01/30/Machine-Learning-Techniques-3/PolynomialKernelProCon.JPG" class="" width="500">

<ol start="2">
<li>Polynomial Kernel<br>Polynomial的好處在經過非線性轉換後，會比linear限制還要少，而且可以透過設定Q來限制模型的維度。缺點在於Polynomial有3個要給定的參數較難選定，而且在計算值時QP不容易解，尤其是γ過大的時候，因此Polynomial比較適合用在心裡有假設而且使用比較小的Q。</li>
</ol>
<img src="/2020/01/30/Machine-Learning-Techniques-3/GaussianKernelProCon.JPG" class="" width="500">

<ol start="3">
<li>Gaussian Kernel<br>Gaussian可以作到無限多維的轉換，所以會比Linear和Polynomial使用限制更少。而且Gaussian的數值會落在0到1之間，所以數值計算難度較低；又因為Gaussian只有一個參數要給定，比起Polynomial要選3個參數，Gaussian可以更容易作參數選擇。Gaussian的壞處在於被轉換到無限多維後，模型沒辦法容易的被解讀；另外他的速度會比單純使用線性還要慢，又因為轉換較複雜，因此使用的不好會造成overfit。</li>
</ol>
<img src="/2020/01/30/Machine-Learning-Techniques-3/OtherValidKernel.JPG" class="" width="500">

<p>Kernel表示一種特徵的相似度，但是不是任何相似度都能用來當作Kernel呢？這邊老師給出必須要滿足對稱性還有postive semi-definite兩種條件才能當作Kernel。</p>
<img src="/2020/01/30/Machine-Learning-Techniques-3/Summary.JPG" class="" width="500">

<p>這一講提到了如何透過kerlnel trick快速計算轉換和內積，避開最複雜的計算，另外也比較了不同的kernel的優點和缺點。</p>
<p>參考資料:<br><a target="_blank" rel="noopener" href="https://www.csie.ntu.edu.tw/~htlin/mooc/doc/203_handout.pdf">Machine Learning Techniques 3</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2020/01/21/Machine-Learning-Techniques-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/21/Machine-Learning-Techniques-2/" class="post-title-link" itemprop="url">機器學習技法(Machine Learning Techniques)第二講筆記</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-01-21 00:00:00" itemprop="dateCreated datePublished" datetime="2020-01-21T00:00:00+08:00">2020-01-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:44:29" itemprop="dateModified" datetime="2021-05-11T23:44:29+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2020/01/21/Machine-Learning-Techniques-2/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/01/21/Machine-Learning-Techniques-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>前面第一堂課在介紹線性的SVM，透過二次規畫找到最大margin的支撐向量來建立更強健的模型。這堂課將會延讀svm並加上非線性轉換的方式，讓SVM不止可以控制模型複雜度，也能結合特微轉換來提高模型效果。</p>
<img src="/2020/01/21/Machine-Learning-Techniques-2/DualSVM.JPG" class="" width="500">

<p>假設透過特徵轉換的方法將SVM轉為非線性，原本的x會被轉換到z空間，但又希望可以在解SVM最佳化問題時，能夠擺脫轉換到z空間高維度的依賴。即將非線性轉換轉成另一個對等問題，不管轉到幾維的空間，都只會有N個變數，變數數量不會和要轉換的維度的有關，這稱為原本SVM的對偶問題(dual problem)。</p>
<img src="/2020/01/21/Machine-Learning-Techniques-2/LagrangeMultiplier.JPG" class="" width="500">

<p>之前篇章在介紹regularization時，引入了Lagrange Multipliers的概念，將原本帶有W長度小於C的限制式，轉成加上Eaug後求最小值，其中λ會被乘上每個系數加進最佳化式子中。因為SVM也是一個有條件的最佳化問題，但是在引入λ時有別於regularization中被當作給定值，在SVM中會被當作未知的值來解最佳化問題。</p>
<img src="/2020/01/21/Machine-Learning-Techniques-2/UnconstrainedSVM.JPG" class="" width="500">

<p>為了把原本有限制式的SVM最佳化問題，在引入Lagrange Multiplier後轉成沒有限制式的問題，會將限制式移項乘上α(即λ在SVM文獻為α)後相加。但是將有限制式的最佳化的問題，轉成沒有限制式的最佳化的問題，是否能夠得到一樣的結果？老師在這裡舉了兩種例子來說明其實會得到一樣的結果。假設選到一組會違反原本限制式的B,W(即α乘上的項次會是正值)，這種最終會在解最小值SVM問題時被逃汰；相反的，如果選到一組符合原本限制式的B,W(即α乘上的項次會負值)，這種解反而在最後會被留下來。因此將限制式轉益沒有限制式的最佳化問題，其實只是把限制藏進求最大值的最佳化中。</p>
<img src="/2020/01/21/Machine-Learning-Techniques-2/LagrangeDualProblem.JPG" class="" width="500">

<p>在轉成無限制式的最佳化問題後，會發現假設在固定α之下，整個最佳化得出來的值會比任一的Lagrange值還要大，甚至取完最大值右邊的不等式也一樣成立，這個min和max互相交換稱為lagrange dual problem。</p>
<img src="/2020/01/21/Machine-Learning-Techniques-2/StrongDualityQP.JPG" class="" width="500">

<p>對於二次規畫，這個duel problem因為滿足強對偶關係，因此求出來的b,w,α值對於左右兩邊的式子都是最佳值，因此可以將原本左邊的最佳化問題，直接轉成解右邊的對偶問題來求最佳值。</p>
<img src="/2020/01/21/Machine-Learning-Techniques-2/DualSimplification1.JPG" class="" width="500">

<p>再進一步化解最佳化問題，如果要得到內部式子的最佳值，因為其為無限制的最佳化問題，因此最佳解會產生在梯度為0，即內部式子微分為0。首先對b微分後加進原本的式子，因為加進去的條件值為0時為最佳解，所以不止不會影響原本求出的最佳解，也可以同時去掉b這個項次。</p>
<img src="/2020/01/21/Machine-Learning-Techniques-2/DualSimplification2.JPG" class="" width="500">

<p>再來對w微分後加進原本的式子，可以將w轉換成α．y．z相乘，在去掉w項次後，也不需要對原本的w取最小值，並轉成一個只有對α作最佳化的問題，得到簡化版的對偶問題。</p>
<img src="/2020/01/21/Machine-Learning-Techniques-2/KKTcondition.JPG" class="" width="500">

<p>前面有提到如果要滿足對偶問題，即b,w,α的解對於左右兩邊的最佳化問題都是最佳值，需要滿足特定條件，這稱為KKT最佳化條件，再會來透過這些條件來解最佳的b,w。</p>
<img src="/2020/01/21/Machine-Learning-Techniques-2/DualFormulationSVM.JPG" class="" width="500">

<p>再來先乘上-1先將最大值問題轉成最小值問題，再把平方向解開後，就可以將每個條件丟進QP來解出最佳值。</p>
<img src="/2020/01/21/Machine-Learning-Techniques-2/Optimal.JPG" class="" width="500">

<p>如果今天在滿足KKT條件下要找到b,w的最佳值，首先要找到最佳的w，因為只有一條條件和w相關所以很容易找到最佳值。在找最佳的b時，共有2個條件和b相關，在第2個條件可以發現如果在α大於0的情況下，y(W．z + b)即需要等於1才能滿足條件，而這個值就是原本要找的最佳解，因此在解對偶問題時，可以同時找到SVM的支稱向量。</p>
<img src="/2020/01/21/Machine-Learning-Techniques-2/SupportVectorRevisit.JPG" class="" width="500">

<p>既然證明在α大於0的情況下，可以找到支撐向量，相反的回到原本的SVM概念，其意義在於如果可以知道有哪些支稱向量，就可以找到最大的margin，其他的點皆可以不管。</p>
<img src="/2020/01/21/Machine-Learning-Techniques-2/RepresentHyperplane.JPG" class="" width="500">

<p>SVM和PLA其實很相似都會找出區分不同類別資料的超平面，差別在於兩者著重的點是不同的，SVM是著重在使用支稱向量表現出來，而PLA是使用分類錯誤(犯錯)的點表現出來。</p>
<img src="/2020/01/21/Machine-Learning-Techniques-2/TwoFormHardMarginSVM.JPG" class="" width="500">

<p>上一講介紹的原始SVM為Hard-Margin SVM，透過求特定放縮後的b,w最佳值，他和要轉換的空間維度有關，適合維度較小的問題；而這一講的SVM是引入Lagrange的SVM，透過找最佳的支稱向量和α來重構邊界，並且和資料量的大小有關，適合資料量小的問題。</p>
<img src="/2020/01/21/Machine-Learning-Techniques-2/AreWeDoneYet.JPG" class="" width="500">

<p>但到目前為止仍然沒辦法完全擺脫處理高度空間維度轉換的問題，因為在QP求Q矩陣時，仍會碰到z向量內積的問題，z向量的維度等同於目前要解的維度。</p>
<img src="/2020/01/21/Machine-Learning-Techniques-2/Summary.JPG" class="" width="500">

<p>這一講提到透過對偶問題來移除對d維度的依賴，並引入Lagrange與KKT條件，透過QP來解出最佳值，並發現解出的最佳值就是支稱向量，並可以用來建立最大margin。下一講將會說明如何真正擺脫和d維度相關的計算。</p>
<p>參考資料:<br/><a target="_blank" rel="noopener" href="https://www.csie.ntu.edu.tw/~htlin/mooc/doc/202_handout.pdf">Machine Learning Techniques 2</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2020/01/09/Machine-Learning-Techniques-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/09/Machine-Learning-Techniques-1/" class="post-title-link" itemprop="url">機器學習技法(Machine Learning Techniques)第一講筆記</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2020-01-09 00:00:00" itemprop="dateCreated datePublished" datetime="2020-01-09T00:00:00+08:00">2020-01-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:41:31" itemprop="dateModified" datetime="2021-05-11T23:41:31+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2020/01/09/Machine-Learning-Techniques-1/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/01/09/Machine-Learning-Techniques-1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>機器學習技法是林軒田老師的開的機器學習後半堂課，主要在延續前面機器學習的基礎理論，並延申出不同的機器學習模型介紹。</p>
<img src="/2020/01/09/Machine-Learning-Techniques-1/CourseOutline.JPG" class="" width="500">

<p>而這堂課主要圍繞在特徵轉換上並分三個面向探討，分別為(1)如果處理大量且高複雜度的特徵轉換(2)找出具有預測性質的特徵來提升模型表現(3)找出資料中的隱藏特徵讓機器學習表現更好。</p>
<img src="/2020/01/09/Machine-Learning-Techniques-1/NoiseTolerateHyperplane.JPG" class="" width="500">

<p>在線性可分的問題中，前面的課程有教過可以透過PLA或是POCKET來找到分開的超平面，但PLA在求解的過程中會得到很多種解，究竟怎麼樣的切線才會是比較好的切線呢？如果以雜訊的容忍度來看，當資料產生時可能會存在或多或少的雜訊(例如從實體感測器訊號收資料時，可能會有震盪或是偏移的現象)，而雜訊是模型過擬合的因素之一。因此如果要讓模型對雜訊的容忍度最大，那麼就要讓超平面能夠離點越遠越好。例如最右邊的超平面能離點最遠，雜訊的容忍度(灰色區域)也最大。</p>
<img src="/2020/01/09/Machine-Learning-Techniques-1/LargestMarginHyperplane.JPG" class="" width="500">

<p>換個方式來，這會是一個最佳化問題，而最好的線須要滿足兩大條件: (1)能作出最大邊界(Margin) (2)可以把不同的類別分對。其中灰色區域的算法，是把每個點和超平面計算距離後取最小距離。</p>
<img src="/2020/01/09/Machine-Learning-Techniques-1/DistanceHyperplane.JPG" class="" width="500">

<p>距離的計算方法，可以想像在平面上有兩個點構成的向量，而w乘上平面上的向量為0，所以w為垂直於平面上的法向量。當有一個x要計算x和平面的距離，即為x和平面上任一個點構成的向量，並對垂直於平面的向量作投影，即為對w方向的投影。</p>
<img src="/2020/01/09/Machine-Learning-Techniques-1/DistanceSeperateHyperplane.JPG" class="" width="500">

<p>因為要找的是一個可以區分出正確類別的分割超平面，所以可以拆掉距離的絕對值，變成乘上y大於0(即為分割正確時值皆會大於0，如果類別分錯乘上y會小於0)</p>
<img src="/2020/01/09/Machine-Learning-Techniques-1/SpecialSpearateHyperplanea.JPG" class="" width="500">

<p>為了簡化式子，假設將式子放縮到最小值會等於1，那margin就會簡化為成1/|w|，並從求margin的最小值，變成分數乘上y的最小值要等於1。而且因為其條件已經滿足大於0，所以可以再拿掉一條分數乘上y要大於0的限制式。</p>
<img src="/2020/01/09/Machine-Learning-Techniques-1/StandardLargeMarginHyperplane.JPG" class="" width="500">

<p>為了再簡化限制式條件，限制式條件可以再從最小值為1，放寬為分數乘上y大於等於1，而且這個放寬並不會違反原本的限制式。老師這裡舉了一個例子，假設找出來的解為大於等於1.126並且不滿足等於1的條件，這時候如果把b和w除上1.126作放縮讓他滿足原本的條件，會因為w變小而使得目標式變大。這個證明在說如果找出來的解不滿於原本等於1的條件下，就不會是最佳解，因此可以對限制式條件作放寬。最後再將最大化的問題轉為求最小值(倒數)，得到最後的簡化的最佳化問題。</p>
<img src="/2020/01/09/Machine-Learning-Techniques-1/SVM.JPG" class="" width="500">

<p>如果要找到最佳的平面，只要找到最靠近平面的點就行了，而這些點被稱為支撐向量(support vector)，就像是這個超平面是由最靠近的點所支撐起來般，這也是SVM的概念。</p>
<img src="/2020/01/09/Machine-Learning-Techniques-1/SolveSVM.JPG" class="" width="500">

<p>因為svm要找到的最佳值是w的兩次函數，且限制式為b和w的一次式，有這樣的限制非常適合使用二次規畫作最佳化。</p>
<img src="/2020/01/09/Machine-Learning-Techniques-1/WhyLargeMarginHyperplane.JPG" class="" width="500">

<p>但為什麼svm可以作的好呢，老師這邊以兩個面向來說明使用svm會讓Ein和Eout越接近，且不容易overfit泛化性更佳。</p>
<p>之前在提到regularization的時候講到，為了讓Ein越小但又不希望造成overfit，於是加上w的限制條件限制其範圍。而svm剛好對調，svm是要讓w的長度越小且限制讓所有的類別的資料分對，所以svm和regularization是一體兩面，svm找出來的灰色區域讓為了容忍雜訊對模型的干擾。</p>
<img src="/2020/01/09/Machine-Learning-Techniques-1/LargeMarginRestrictDichotomy.JPG" class="" width="500">

<p>假設平面上有三個點，如果是原本的pla，在任意切線上可以找到所有的類別組合(共八種)。但使用svm考量到需要維持最大特定margin區域情況，所以沒有辦法作出所有的類別排列組合。在vc維的介紹有講到，如果能作出的dichotomies越少，vc維就越小，Ein和Eout就會越接近，即泛化能力越好。</p>
<img src="/2020/01/09/Machine-Learning-Techniques-1/BenefitSVM.JPG" class="" width="500">

<p>從上面兩個面向來看，svm可以帶來本質上泛化性更佳的好處，並且在加上特徵轉換的方法後，non-linear的svm可以同時辦到將Ein(即分對不同類別)與Eout(泛化能力)作好。</p>
<img src="/2020/01/09/Machine-Learning-Techniques-1/SummaryLecture1.JPG" class="" width="500">

<p>這一講主要在說明如何從邊界分類問題延申出最大margin提供更強健的方法來容忍雜訊，並作svm最大margin的最佳化式子推導，最後提到最大margin帶來本值上的好處在於可以提高更佳的泛化能力。</p>
<p>參考資料:<br/><a target="_blank" rel="noopener" href="https://www.csie.ntu.edu.tw/~htlin/mooc/doc/201_handout.pdf">Machine Learning Techniques 1</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2019/03/14/machine-learning-on-gcp-feature-engineering2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/03/14/machine-learning-on-gcp-feature-engineering2/" class="post-title-link" itemprop="url">Machine Learning on GCP - Feature Engineering(下)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2019-03-14 00:00:00" itemprop="dateCreated datePublished" datetime="2019-03-14T00:00:00+08:00">2019-03-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:40:33" itemprop="dateModified" datetime="2021-05-11T23:40:33+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2019/03/14/machine-learning-on-gcp-feature-engineering2/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/03/14/machine-learning-on-gcp-feature-engineering2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a href="https://qiubite31.github.io/2019/03/05/machine-learning-on-gcp-feature-engineering/">上一篇</a>提到一些基本的Feature Engineering概念與方法，這一篇則是會說明當要使用類別型的特徵來訓練機器學習模型時的技巧。</p>
<h2 id="Categorical"><a href="#Categorical" class="headerlink" title="Categorical"></a>Categorical</h2><img src="/2019/03/14/machine-learning-on-gcp-feature-engineering2/Categorical.JPG" class="" width="500">

<p>如果遇到類別型的特徵又需要拿進來訓練模型，則可以用one-hot encoding來處理。例如當今天要對商品銷售預測建立模型時，想要把員工拿進來考量，也許不同員工對顧客的服務上會影響到商品銷售。雖然員工的編號是數值，但其並不存在實際上數值的意義，這時候就可以透過one-hot來處理，並以多個欄位變數表示來將每個值轉成0/1表示的稀疏向量。</p>
<img src="/2019/03/14/machine-learning-on-gcp-feature-engineering2/CustomerRating.JPG" class="" width="500">

<p>有時候在某些資料則可以當作連續型處理，也可以使用one-hot來處理。例如顧客的評分，如果你認為4分和2分是差距很大的，這時也可以依個人考量當作類別型處理。要特別注意的是，如果今天顧客沒有提供評分資料，在處理missing value上，第一種數值型的處理方法是使用另一個欄位來紀錄是否有收到評分(1/0)，並維持評分值為0；第二種類別型處理方法則將所有one-hot變數設為0，並一樣透過另一欄位紀錄是否有評分，注意不要使用自己的特別編碼(magic number)來處理。</p>
<h2 id="Feature-Cross"><a href="#Feature-Cross" class="headerlink" title="Feature Cross"></a>Feature Cross</h2><img src="/2019/03/14/machine-learning-on-gcp-feature-engineering2/FeatureCross.JPG" class="" width="500">

<p>假設今天要建立模型來判斷車輛是否為計程車，而使用的特徵只有兩個，分別為車倆的顏色和車輛所屬城市。假設透過簡單的線性模型來作訓練，在調整權重的過程中，都沒辦法有好的辨識效果。因為模型在調整黃色和白色的權重時，當它看到黃色在紐約是計程車，提高了黃色的權重，但這反而造成所有黃色車倆比較容易判斷成計程車，這是不對的；相同的如果模型提高了紐約的權重，這也會造成所有紐約的車倆都容易判斷成計程車，這一樣是不對的。</p>
<p>這時候則可以嘗試將兩個變數結合變成第三個變數，並透過one-hot encoding來處理，而在訓練過程中就會將黃色X紐約的組合單獨調整權重，可以避開原本的問題。例如在預測計程車車資問題中，雖然知道在上下班時間的旅程時間會比較長，車資也可能比較高，但是這時可以不特別作新增rule的處理(例如標注某天的某時段為上下班時間)，而是直接將Day of Week和24小時作feature cross建立組合。</p>
<h2 id="Bucketize"><a href="#Bucketize" class="headerlink" title="Bucketize"></a>Bucketize</h2><img src="/2019/03/14/machine-learning-on-gcp-feature-engineering2/DiscreteFloat.JPG" class="" width="500">

<p>以加州的房價預測來說，如果觀察緯度這個特徵會發現有兩個高峰，一個是舊金山灣區，另一個則是洛杉磯大都市，這時就可以透過資料分群(bin)來拆成100個bucket，轉成類別型資料來訓練模型。注意在預測時也要透過資料前處理來將資料作bucketize。</p>
<h2 id="Wide-and-Deep"><a href="#Wide-and-Deep" class="headerlink" title="Wide and Deep"></a>Wide and Deep</h2><img src="/2019/03/14/machine-learning-on-gcp-feature-engineering2/DenseAndSparse.JPG" class="" width="500">

<p>到這邊就會遇到一個問題，在銷售遇到時會有價格和員工兩種不同特性的變數，價格是密集的(Dense)的連續型變數，而員工編號是透過one-hot產生稀疏(Sparse)的類別型變數。而在使用類神經網路訓練模型時，因為在0在乘上權重還是為0，所以稀疏的矩陣可能造成訓練過程中收斂在區域最佳解跳不出來。但是以上面說的計程車例子，其實線性模型是比較容處理的。</p>
<img src="/2019/03/14/machine-learning-on-gcp-feature-engineering2/EatAll.JPG" class="" width="500">

<p>因此在訓練模型時，可以嘗試合併兩種方法，透過類神經網路來使用連續型變數訓練深度(Deep)結果，再和類別型變數透過線性方式串聯(Wide)，這即是一個wid-and-deep架構的神經網路模型。</p>
<p>參考資料:<br/><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/serverless-data-analysis-bigquery-cloud-dataflow-gcp/home/welcome">Data Engineering on Google Cloud Platform 4 - Serverless Data Analysis with Google BigQuery and Cloud Dataflow</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2019/03/05/machine-learning-on-gcp-feature-engineering/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/03/05/machine-learning-on-gcp-feature-engineering/" class="post-title-link" itemprop="url">Machine Learning on GCP - Feature Engineering(上)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2019-03-05 00:00:00" itemprop="dateCreated datePublished" datetime="2019-03-05T00:00:00+08:00">2019-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:40:22" itemprop="dateModified" datetime="2021-05-11T23:40:22+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2019/03/05/machine-learning-on-gcp-feature-engineering/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/03/05/machine-learning-on-gcp-feature-engineering/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>前陣子在上Coursera的Data Engineering on Google Cloud Platform這個系列課程，其中在Serverless Machine Learning with Tensorflow on Google Cloud Platform這週內有一個Feature Engineering單元，裡面展示了如何透過Feature Engineering來提升模型的表現。上完後覺得裡面提到一些關於Feature Engineering的技巧，決定還是找時間把筆記寫下來。</p>
<img src="/2019/03/05/machine-learning-on-gcp-feature-engineering/GoodFeature.JPG" class="" width="500">

<p>好的特徵必須要和預測目標值是有相關的，對於特徵和預測值之間，需要有合理的假設，而不是隨意丟任意的資料進來，就希望特徵和預測值間具有關聯性，否則會落入Data Dredge的問題。Data Dredge意指可能會從大量資料中找到另人意外的相關性，這並不是我們想要的結果。(例如荷蘭的研究中指出一個地方送子鳥被看到的數量，和9個月後嬰兒出生的數量相關)</p>
<h2 id="Causality"><a href="#Causality" class="headerlink" title="Causality"></a>Causality</h2><img src="/2019/03/05/machine-learning-on-gcp-feature-engineering/Causality.JPG" class="" width="500">

<p>好的特徵特性是要使用預測當下能夠掌握的資料當作特徵，例如當你要使用每日的銷售資料當作特徵值，但是這些資料可能需要一個月的資料才會產生，而不是及時會被收集到資料倉儲。像這種可能因為資料延遲造成在預測時無法取得完整的資料將可能造成模型失效。所以在訓練模型時，請確保這些特徵在預測時是可以完整取得的，否則不要使用在模型中。</p>
<h2 id="Numeric-amp-Magnitude"><a href="#Numeric-amp-Magnitude" class="headerlink" title="Numeric &amp; Magnitude"></a>Numeric &amp; Magnitude</h2><img src="/2019/03/05/machine-learning-on-gcp-feature-engineering/Numeric.JPG" class="" width="500">

<p>因為在機器學習的過程中，會對輸入的資料作許多的運算，因此使用的特徵必須要為數值形態，且其數值是有大小意義的(例如coupon提供的打折數20%和10%存在折數大小關係)。</p>
<h2 id="Enough-examples"><a href="#Enough-examples" class="headerlink" title="Enough examples"></a>Enough examples</h2><img src="/2019/03/05/machine-learning-on-gcp-feature-engineering/EnoughData.JPG" class="" width="500">

<p>好的特徵需要有足夠的資料，以講者的個人經驗來說，如果一個特徵中每個值出現至少5筆，才會將這個特徵用來訓練模型。舉例來說，有一個類別為自動交易，需要有足夠的詐欺/非詐欺資料才有辦法訓練出有效的機器學習模型。如果今天只有3筆自動交易資料，且3筆都是非詐欺，這樣數量的資料可能就無法訓練出可用的機器學習模型。我想這裡的用意是指一個特徵如果相似度太高，可能造成沒有鑑別度；例如在分類問題中，特徵在每個類別的值都一樣或相似，那麼這樣的特徵可能對分類問題沒辦法貢獻太多資訊，使用決策樹來切分也會找不到好的切點。</p>
<p>參考資料:<br/><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/serverless-data-analysis-bigquery-cloud-dataflow-gcp/home/welcome">Data Engineering on Google Cloud Platform 4 - Serverless Data Analysis with Google BigQuery and Cloud Dataflow</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2018/05/26/%E8%AE%93%E6%88%91%E5%80%91%E7%94%A8Counter%E4%BE%86%E8%A8%88%E7%AE%97%E6%95%B8%E9%87%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/05/26/%E8%AE%93%E6%88%91%E5%80%91%E7%94%A8Counter%E4%BE%86%E8%A8%88%E7%AE%97%E6%95%B8%E9%87%8F/" class="post-title-link" itemprop="url">讓我們用collections中的Counter來計算數量</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2018-05-26 00:00:00" itemprop="dateCreated datePublished" datetime="2018-05-26T00:00:00+08:00">2018-05-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:38:31" itemprop="dateModified" datetime="2021-05-11T23:38:31+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2018/05/26/%E8%AE%93%E6%88%91%E5%80%91%E7%94%A8Counter%E4%BE%86%E8%A8%88%E7%AE%97%E6%95%B8%E9%87%8F/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/05/26/讓我們用Counter來計算數量/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Python的collection模組裡面其實包含了許多非常實用的資料結構，比如之前介紹過的<br/><a href="/2017/03/02/%E4%BD%BF%E7%94%A8collections%E4%B8%AD%E7%9A%84namedtuple%E4%BE%86%E6%93%8D%E4%BD%9C%E7%B0%A1%E5%96%AE%E7%9A%84%E7%89%A9%E4%BB%B6%E7%B5%90%E6%A7%8B/">namedtuple</a>。今天要談的是Counter，Counter是一個dict的子類別，用來對hashable的物件作計算。</p>
<p>比如說我們今天要來幫公司裡面每個不同的team訂飲料好了，以下簡易一點不接受客製化調味，在建立Counter可以有以下幾種方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用mapping建立Counter，輸入一個dict</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>team1 = Counter(&#123;<span class="string">&#x27;BlackTea&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;GreenTea&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;MilkTea&#x27;</span>: <span class="number">1</span>&#125;)</span><br><span class="line"><span class="comment"># 使用iterable建立Counter，輸入一個list</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>team2 = Counter([<span class="string">&#x27;BlackTea&#x27;</span>, <span class="string">&#x27;BlackTea&#x27;</span>, <span class="string">&#x27;BlackTea&#x27;</span>, <span class="string">&#x27;MilkTea&#x27;</span>, <span class="string">&#x27;MilkTea&#x27;</span>, <span class="string">&#x27;MilkTea&#x27;</span>])</span><br><span class="line"><span class="comment"># 使用keyword參數建立Counter，輸入key-value組合</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>team3 = Counter(GreenTea=<span class="number">3</span>, MilkTea=<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>team1[<span class="string">&#x27;BlackTea&#x27;</span>]  <span class="comment"># 可以直接當作dict存取</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>team2[<span class="string">&#x27;GreanTea&#x27;</span>]  <span class="comment"># 取出不存在的item即為0</span></span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>可以看到透過Couter能透過三種不同的方法來建立，就看使用的情境比較適合哪一種。而Counter也可以直接就當作dict取出值，特別是如果取出不存有的item，其計數會為0，並不會出現dict的KeyError。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>team1 + team2  <span class="comment"># 使用加法運算符</span></span><br><span class="line">Counter(&#123;<span class="string">&#x27;BlackTea&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;MilkTea&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;GreenTea&#x27;</span>: <span class="number">2</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>team2 - team3  <span class="comment"># 使用減法運算符</span></span><br><span class="line">Counter(&#123;<span class="string">&#x27;BlackTea&#x27;</span>: <span class="number">3</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>team2 &amp;amp; team3  <span class="comment"># 運算兩者的交集</span></span><br><span class="line">Counter(&#123;<span class="string">&#x27;MilkTea&#x27;</span>: <span class="number">3</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>team2 - team3  <span class="comment"># 運算兩者的聯集</span></span><br><span class="line">Counter(&#123;<span class="string">&#x27;MilkTea&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;GreenTea&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;BlackTea&#x27;</span>: <span class="number">3</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>Counter也可以支援不同的運算符來對Counter物件進行操作，來達成對不同集合元件的運算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>drink_order = team1 + team2 + team3</span><br><span class="line">Counter(&#123;<span class="string">&#x27;MilkTea&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;BlackTea&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;GreenTea&#x27;</span>: <span class="number">5</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">list</span>(drink_order)  <span class="comment"># 將Counter的items轉成list</span></span><br><span class="line">[<span class="string">&#x27;BlackTea&#x27;</span>, <span class="string">&#x27;GreenTea&#x27;</span>, <span class="string">&#x27;MilkTea&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">set</span>(drink_order)  <span class="comment"># 將Counter的items轉成set</span></span><br><span class="line">[<span class="string">&#x27;BlackTea&#x27;</span>, <span class="string">&#x27;GreenTea&#x27;</span>, <span class="string">&#x27;MilkTea&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">dict</span>(drink_order)  <span class="comment"># 將Counter的items轉成一般dict</span></span><br><span class="line">&#123;<span class="string">&#x27;BlackTea&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;GreenTea&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;MilkTea&#x27;</span>: <span class="number">7</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>drink_order.items()  <span class="comment"># 取出item pairs</span></span><br><span class="line">dict_items([(<span class="string">&#x27;MilkTea&#x27;</span>, <span class="number">7</span>), (<span class="string">&#x27;BlackTea&#x27;</span>, <span class="number">6</span>), (<span class="string">&#x27;GreenTea&#x27;</span>, <span class="number">5</span>)])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>drink_order.keys()  <span class="comment"># 取出key值</span></span><br><span class="line">dict_keys([<span class="string">&#x27;MilkTea&#x27;</span>, <span class="string">&#x27;BlackTea&#x27;</span>, <span class="string">&#x27;GreenTea&#x27;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>drink_order.values()  <span class="comment"># 取出value值</span></span><br><span class="line">dict_values([<span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">sum</span>(drink_order.values())  <span class="comment"># 取出value值並加總得到全部數量</span></span><br><span class="line"><span class="number">18</span></span><br></pre></td></tr></table></figure>
<p>因為Counter是dict的一個字類別，所以基本上他可以辦到原本dict可以作到的資料轉型，包含透過items()來逐一取出每個pair，或是直接可以使用values()並加總來計算出Counter裡面總計有多少數量的東西。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span>(drink_order.elements())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="string">&#x27;BlackTea&#x27;</span>, <span class="string">&#x27;BlackTea&#x27;</span>, <span class="string">&#x27;BlackTea&#x27;</span>, <span class="string">&#x27;BlackTea&#x27;</span>, <span class="string">&#x27;BlackTea&#x27;</span>, <span class="string">&#x27;BlackTea&#x27;</span>, <span class="string">&#x27;GreenTea&#x27;</span>, <span class="string">&#x27;GreenTea&#x27;</span>, <span class="string">&#x27;GreenTea&#x27;</span>, <span class="string">&#x27;GreenTea&#x27;</span>, <span class="string">&#x27;GreenTea&#x27;</span>, <span class="string">&#x27;MilkTea&#x27;</span>, <span class="string">&#x27;MilkTea&#x27;</span>, <span class="string">&#x27;MilkTea&#x27;</span>, <span class="string">&#x27;MilkTea&#x27;</span>, <span class="string">&#x27;MilkTea&#x27;</span>, <span class="string">&#x27;MilkTea&#x27;</span>, <span class="string">&#x27;MilkTea&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>drink_order.most_common(<span class="number">3</span>)</span><br><span class="line">[(<span class="string">&#x27;MilkTea&#x27;</span>, <span class="number">7</span>), (<span class="string">&#x27;BlackTea&#x27;</span>, <span class="number">6</span>), (<span class="string">&#x27;GreenTea&#x27;</span>, <span class="number">5</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>drink_order.most_common(<span class="number">2</span>)</span><br><span class="line">[(<span class="string">&#x27;MilkTea&#x27;</span>, <span class="number">7</span>), (<span class="string">&#x27;BlackTea&#x27;</span>, <span class="number">6</span>)]</span><br></pre></td></tr></table></figure>
<p>Counter還支援兩個函式，elements可以幫你展開所有的items。而most_common則會使用數量作排序，並且可以指定要排出前幾名。</p>
<p>Counter其實使用上非常方便，讓我們看一下Counter在實戰上可以拿來解什麼樣的問題，以下選兩個LeetCode的題目來看看Counter可以怎麼樣被使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;389. Find the Difference</span></span><br><span class="line"><span class="string">Given two strings s and t which consist of only lowercase letters.</span></span><br><span class="line"><span class="string">String t is generated by random shuffling string s and then add one more letter </span></span><br><span class="line"><span class="string">at a random position. Find the letter that was added in t.</span></span><br><span class="line"><span class="string">Input:</span></span><br><span class="line"><span class="string">s = &quot;abcd&quot;</span></span><br><span class="line"><span class="string">t = &quot;abcde&quot;</span></span><br><span class="line"><span class="string">Output: e (&#x27;e&#x27; is the letter that was added.)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findTheDifference</span>(<span class="params">self, s, t</span>):</span></span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">    s_counter = Counter(s)</span><br><span class="line">    t_counter = Counter(t)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(t_counter - s_counter)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>LeetCode-389是給定兩個字串s和t，其中要判斷t比s多了哪個字元。這時候就可以直接將s和t分別以iterable的方式建立兩個Counter，而我們知道t會比s多一個字元，於是再透過減法來找到t和s的差集後，輸出是哪個item即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;819. Most Common Word</span></span><br><span class="line"><span class="string">Given a paragraph and a list of banned words, return the most frequent word </span></span><br><span class="line"><span class="string">that is not in the list of banned words. It is guaranteed there is at least </span></span><br><span class="line"><span class="string">one word that isn&#x27;t banned, and that the answer is unique.</span></span><br><span class="line"><span class="string">Words in the list of banned words are given in lowercase, and free of punctuation.</span></span><br><span class="line"><span class="string">Words in the paragraph are not case sensitive.  The answer is in lowercase.</span></span><br><span class="line"><span class="string">Example:</span></span><br><span class="line"><span class="string">Input: </span></span><br><span class="line"><span class="string">paragraph = &quot;Bob hit a ball, the hit BALL flew far after it was hit.&quot;</span></span><br><span class="line"><span class="string">banned = [&quot;hit&quot;]</span></span><br><span class="line"><span class="string">Output: &quot;ball&quot;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mostCommonWord</span>(<span class="params">self, paragraph, banned</span>):</span></span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">    words = re.sub(<span class="string">&quot;[!|?|&#x27;|,|;|.]&quot;</span>, <span class="string">&#x27;&#x27;</span>, paragraph).lower().split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> (Counter(word <span class="keyword">for</span> word <span class="keyword">in</span> words <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> banned).most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>LeetCode-819是給定一個段落，還有被禁掉的詞清單，接著輸出出現字詞次數最多次，而且沒有被禁掉的詞。這邊先將所有詞轉成小寫，接著再透過空白切分後，將特殊符號取代掉只留下字詞。再來就可以將不存在於禁用清單的詞拿來建立Counter，然後使用most_common取出次數最高的第一名，接著再輸出是哪個詞即可。</p>
<p>Counter其實在計算次數的時候非常實用，特別是在項目很多的情況下，而且還需要作加減運算時，可以嘗試使用Counter來解問題，絕對可以幫助你省下很多的時間的！</p>
<p>參考資料:<br/><a target="_blank" rel="noopener" href="https://docs.python.org/2/library/collections.html#collections.Counter">Python docs - Counter dict subclass for counting hashable objects</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2018/02/27/andrew-deep-learning-course5-week3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/27/andrew-deep-learning-course5-week3/" class="post-title-link" itemprop="url">Andrew深度學習課程五 - Sequence Model第3周筆記</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2018-02-27 00:00:00" itemprop="dateCreated datePublished" datetime="2018-02-27T00:00:00+08:00">2018-02-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:34:12" itemprop="dateModified" datetime="2021-05-11T23:34:12+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2018/02/27/andrew-deep-learning-course5-week3/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/02/27/andrew-deep-learning-course5-week3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在RNN的應用中，有一種是sequence to sequence模型，像是在語言翻譯問題上，要把長度為5的法文翻譯成長度為6的英文。首先先透過一個encoder將每個法文字詞輸入進RNN模型，再透過decoder逐一輸出英文字詞直到結尾，這樣的模型被證實只要使用足夠大量的法文和英文句子就可以完成。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/sequence_to_sequence.JPG" class="" width="500">

<p>另一個類似的應用是給一張照片，然後自動給予這張照片適當的標題，這時可以透過CNN來建立一個encoder，接著再透過RNN建立一個decoder來產生適當的標題。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/image_caption.JPG" class="" width="500">

<p>第一週有學到language model，會計算產生句子的機率。而在翻譯問題中的encoder部分和language model非常相似，只是在輸入不是向量0，而是一個encoder的網絡。翻譯問題其實很像是一個conditional language model，即給定輸入翻譯前的句子X之下，輸出不同翻譯結果的機率。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/conditional_language_model.JPG" class="" width="500">

<p>我們並不希望直接對這個分布輸出作隨機的取樣，因為取樣的結果容易很不穩定，有時取到好的翻譯結果，有時取到不好的翻譯結果。所以在使用這個模型的時候，需要使用一個演算法來計算出給定X輸出譯翻結果Y的最大條件機率。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/most_likely_translate.JPG" class="" width="500">

<p>一個方法是使用貪婪搜尋法(Greedy Search)，即先根據conditional language model來產生第一個詞，接著在後面的翻譯部份每一次再依據前個詞選取最大機率來產生下個詞。但我們實際上希望的是找到輸出翻譯結果中每個詞Yi最大的聯合機率(Joint Probability)，而不是每次都產生一個機率最大的詞，比如說法文翻譯到英文的問題中，如果前兩個字的翻譯結果開頭都是Jane is…，但是going這個字在英文上比起visiting還常見，所以用這個方法會造成第3個詞比較容易接going，但是這並不是一個最佳化的翻譯結果。但在英文可選的詞非常多，所以會使用approximate搜尋法來嘗試找到最大的條件機率。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/greedy_search.JPG" class="" width="500">

<p>Beam Search演算法的特性在於不會像之前一樣只找一個最大的機率，而是會存下前B個最大的機率值在記憶體裡面。比如說第1個詞已經找出是in, jane和september，再來會繼續再找出第1個詞是in的情況下，第2個是哪個詞。這時會計算出在給定輸入X且第一個詞是in之下，哪個詞的機率最大，即P(Y2|X,Y1)。再將機率乘上P(Y1|X)就可以得到給定X之下，前兩個詞的機率P(Y1,Y2|X)。然後存下前三個最大的可能性，這時可能會存下in september, jane is和jane visiting，再繼續往下找第3個詞。這時也就代表演算法認為september不會是第一個詞。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/beam_search_step2.JPG" class="" width="500">

<p>接著beam search再從前兩個詞的三組詞往下展開來，並找出前三個詞最大機率的三組保留，透過這個方法持續到接到結尾EOS並停止。要注意的是如果把B設定成1的話，就等同於前面介紹的Greedy Search。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/beam_search2.JPG" class="" width="500">

<p>有些不同的方法可以讓beam search可以得到更好的結果，其中一個是length normalization。beam search會持續連乘使得機率值變很小，這可以透過取log相加取代，因為logP(y|x)和P(y|x)兩者取最大值將會有一樣的結果。當句子很長時，可以透過改變目標函式的方法來解決這個問題。因為取log後為&lt;=1，所以當詞越多值也就會越小，一個方法是除上詞的數量來作正規化，這可以大幅的降低因為句子太長所造成的懲罰(Penalty)效果。另外為了要讓這個正規化效果更加的平滑，可以將Ty取α次方，比如說設定成0.7(1為完整的正規化，0為不作正規化)</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/length_normalization.JPG" class="" width="500">

<p>但B值到底該如何選擇呢？B值越大會可以留下更多的組合來找到更好的結果，但卻訓練的時間會更久，而且也會使用更多的記憶體。Andrew這邊建立在production系統B為10就可以了，但是在研究的系統上，可以取更大的B來計算不同的組合，多次作嘗試。</p>
<p>因為翻譯問題不是像是圖像辨識一樣有正確的答案可以用accuracy來衡量，翻譯結果可能可以有多個一樣好的結果，這個時候可以使用Bleu(Biligual Evaluation Understudy) score來作訓練結果的衡量。<br/>Precision會比對機器翻譯出來的結果和人翻譯的結果，看機器翻譯的每個詞，是否都有出現在不同的答案reference中。比如果機器翻譯出來有7個詞，其中the這個詞都出現在reference中，所以precision為7/7。但這樣的計算顯然不夠精確。Modified Precision則會給予每個詞不同權重，像是the在reference 1出現2次，在reference 2出現1次，因此會給它權重為2，那邊Modified Precision將會修正2/7。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/evaluation_machine_translation.JPG" class="" width="500">

<p>但是衡量並不會只看單一詞的結果，比如說使用兩個詞為一組(Bigram)來作計算的話，會先透過bigram來取出所有組合，並計算每個組合在機器翻譯結果出現次數，再來計算每個組合在所有的reference中有沒有出現(1或0)，再來則將reference的數量除上機器翻譯bigram出現次數來計算 bigram的precision。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/bleu_score_bigram.JPG" class="" width="500">

<p>所以在使用N-gram取組合時，即可以歸納出一個公式，如果機器翻譯出來的結果和其中一組reference相同時，modified precision就會等於1。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/bleu_score_unigram.JPG" class="" width="500">

<p>把多個n-grams分數綜合起來，可以計算出一個綜合的分數再乘上BP。因為當機器的翻譯結果過短時，會造成大多的字存在reference中，使得modified precision會偏高，此時會引入BP(Brevity Penalty)的概念，當機器翻譯的長度比起reference還長時，BP為1即不給予懲罰分數；反之給予一個懲罰分數。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/bleu_detail.JPG" class="" width="500">

<p>實際上人在進行翻譯時，並不會把整段都看完再翻譯，而是先看一小段翻完再往後看另一小段。而且當句子越長時，Bleu score會逐漸降低，這時如果使用Attention Model來幫助每一小段作翻譯的話，則可以解決這個問題。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/long_sequence_problem.JPG" class="" width="500">

<p>雖然attention model是用在比較長的句子，但這邊使用短句來作說明。這裡encoder使用bidirectional RNN，在decoder時透過另一個RNN來完成翻譯。在產生第1個詞時會透過α來表示要產生這個詞應該要注意的資訊量有多少。接著再產生第2個詞時，會產生一組新的attention weight來表示產生第兩個詞時該注意的資訊量，並加上前一個產生的詞。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/attention_model_intuition.JPG" class="" width="500">

<p>在建立attention model時，encoder使用的bidirectional RNN會貢獻兩個activation值，此值即為α權重。接著在decoder產生每個Y值時會輸入C，這個C即為α值的加總，用來表示要產生的Y值需要注意多少的資料量，而這個α會取softmax確保其加總為1。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/attention_model.JPG" class="" width="500">

<p>所以在產生Y時主要有兩個輸入，一個是上個hidden狀態S&lt; t-1 &gt;，一個是要注意的資訊量a &lt; t’ &gt;，透過建立這個小的神經網路來使用梯度下降作訓練。但attention model的缺點就是他的計算成本較高，複雜度為Tx * Ty。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/computing_attention.JPG" class="" width="500">

<p>seqence to sequence可以被使用在文字的正規化，把不同型態的文字表達轉成一個相同的表示方法。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/attention_example.JPG" class="" width="500">

<p>語音辨識(Speech Recognition)也是一種sequence to sequence的問題，輸入一段語音後期望可以輸出一段正確的辨識結果。傳統的語音辨識方法會使用音位(Phoneme)當作基本的單位來表示語音結果，但在end to end的深度學習方法，已經不再需要使用這種方法。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/speech_recognition_problem.JPG" class="" width="500">

<p>一個有效的方法是使用CTC cost，語音辨識因為每秒可能有多次的取樣，所以輸入的資料會很大，光是10秒的語音就可以變成上1000的輸入，但是輸出的辨識結果並不會有上千的輸出。CTC可以協助幫輸出的結果作適當的collapse，其會對重複的字作collapse像是重複的t, e和q來產生較短的結果。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/CTC_cost.JPG" class="" width="500">

<p>這裡以trigger word偵測系為例來說明語音辨識問題，trigger word常被使用在一些智慧管家的產品當中，像是Google Home或是Amazon Echo。在訓練過程中會輸入語音，並在出現trigger word時的目標label設定為1，其它部份為0。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/trigger_word.JPG" class="" width="500">

<p>參考資料:<br/><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/nlp-sequence-models">Deep Learning Course 5 - Sequece Models</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2018/02/18/andrew-deep-learning-course5-week2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/18/andrew-deep-learning-course5-week2/" class="post-title-link" itemprop="url">Andrew深度學習課程五 - Sequence Model第2周筆記</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2018-02-18 00:00:00" itemprop="dateCreated datePublished" datetime="2018-02-18T00:00:00+08:00">2018-02-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:31:20" itemprop="dateModified" datetime="2021-05-11T23:31:20+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2018/02/18/andrew-deep-learning-course5-week2/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/02/18/andrew-deep-learning-course5-week2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在上一週的課程中，我們使用了one-hot結合詞庫來將句子作量化處理。但是使用one-hot的缺點是每個詞被視為獨立的，也就是無法衡量出兩個詞之間的相似或與重要性(兩個one-hot vector內積都會為0)。所以當你想知道apple和orange之間的關係比起apple和king還接近，就無法單純的使用one-hot來作文字的量化。其中一個方法是可以用不同的特性來作量化，比如說使用性別、是否為食物等等的特性，這個時候便可以辯識出具有相同性質的詞。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/word_embeddings_feature.JPG" class="" width="500">

<p>這種量化文字的方法又可以被稱為Word Embedding，像是每個詞會嵌在高維度的特徵向量中，即每個詞都可以在高維度的特徵找到一個能表示它的位置，而如果要將多個維度的特徵壓縮到兩維來表示，可以使用t-SNE演算法來找到相同的詞會彼此較相近。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/word_embeddings_visualize.JPG" class="" width="500">

<p>使用Word Embedding的好處除了可以判斷出兩個詞之間的相關係，也對Transfer learning有很大的幫助，即使用已經訓練好的word embeddig來用在新的任務。在使用word embedding於transfer learning有幾種方法。1. 從大量的詞庫(1-3B)中進行訓練(或是下載已被訓練過的) 2. 使用原本已建立的embedding並transfer到新的而且訓練資料集較小(1-3k)的任務 3. 持續的使用新的資料來對embeddings進行finetune。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/word_embeddings_transfer.JPG" class="" width="500">

<p>Word Embeddings也可以作到比擬(analogy)，比如說Man之於Woman等同於King之於Queen，這可以透過向量相減來找到相似的結果。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/analogy.JPG" class="" width="500">

<p>實際上要基於Man與Woman的關係來找到King和某個詞，可以透過向量相似度的計算來找到，只要找到King和哪一個詞的相似度與Man和Woman的相似度最接近即可。這裡的相似度計算是從原始的高維度(~300D)來的，雖然可以透過t-SNE來將高維度使用非線性的轉換到2維空間作視覺化，但如果使用了轉換的2維來計算反而會失真，所以在實際上的相似度計算會使用原始的高維度向量空間。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/analogy_vector.JPG" class="" width="500">

<p>一個常用的相似度計算是Cosine similarity，即計算兩個向量的角度來判斷兩個相向之間的相似度。Cosine similarity也經常被用在文本分析中，透過計算代表兩筆資料的高維度向量，來評估兩筆資料的相似度。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/cosine_similarity.JPG" class="" width="500">

<p>Word Embedding的學習最終會得到一個Embedding Matrix，凡是把這個矩陣乘上某一個詞的one-hot向量，所得到的即是代表這個詞的embedding。因為one-hot是高維度而且幾乎都是0的稀疏矩陣，所以在實務上並不會真的相乘，而且透過特別的方法來找到詞的embeddings。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/embedding_matrix.JPG" class="" width="500">

<p>在word embeddings的學習演算法中，過去往往會使用很多很複雜的演算法來學習，但後來卻發現很多簡單的方法就可以達到很好的效果。在複雜的方法中，比如說要預測一句話的下一個詞，可以把每個詞的word embeddings輸入到一個類神經網路學習，最後訓練出我們要的embedding matrix(E)。如果有300維度，那麼要輸入到網路裡面的維度就會多達300乘6共1800維，也可以選擇只看前4個詞來學習，那麼維度就會降到300乘4共1200維。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/other_context_target.JPG" class="" width="500">

<p>不過事實上用簡單一點的模型也可以訓練出好的效果，比如說只用預測詞的前一個字，或是最接近預測詞前一個詞的詞(又稱為skip gram)來訓練word embeddings matrix。</p>
<p>這裡介紹Word2Vec的Skip-grams模型，Skip-grams在選擇Context與Target組合時，針對Target可透過隨機挑選的方式選出不同的組合，比如說Context詞的+-5~10個詞來選出Target詞。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/skip_grams.JPG" class="" width="500">

<p>再來將這些對應的組合透過matrix E轉成word embeddings後丟入神經路網學習，接著經過softmax後計算loss function來訓練出matrix E。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/skip_grams_model.JPG" class="" width="500">

<p>但這個方法的主要缺點在於需要大量的計算，因為在計算softmax時，分別需要加總所有詞，如果詞的數量越多，那麼速度就會越慢，因此很難擴充到大型的訓練詞集。一個解法是使用hierarchical softmax，先拆出詞位於哪個部份(前或是後五千詞)，接著再接續往下作二元拆解。在tree的結構可以把常出現的通用詞放在上半部，這樣就可以較快的被找到。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/softmax_problem.JPG" class="" width="500">

<p>因為skip-grams在計算softmax需要花費大量運算，使用Negative Sampling則可以避免這個問題。Negative Sampling首先會先建立postive pair和negaive pair，比如說orange和juice會一起出現，就給予label 1，然後再隨機選出其他K個詞和orange組成negative pair給予label 0(即便某些時候negative pair組合真的有一起出現也沒關係)。最後將這些組合透過監督式學習訓練模型。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/negative_sampling.JPG" class="" width="500">

<p>接著可以使用logistic regression來計算在給定label為1時，postive pair組合的機率，並使用K+1組資料進行訓練，然後再將這個分類器應用在10000個詞作二元分類。因此比起原本作skip-grams要一次訓練10000組資料，使用negatiev sampling的方法只需要用少量的資料訓練，再一次用在多組詞的分類。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/negative_sampling_model.JPG" class="" width="500">

<p>但是該如何去選擇negative sampling呢？一個方法可以從詞出現的頻率選起，但是很容易選到停用詞(stop word)；或是假定他是均勻分配，使用1/|V|來選擇，而在這篇論文裡面是用了每個詞在訓練詞庫中出現的3/4次方來計算機率作選擇。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/negative_sampling_selection.JPG" class="" width="500">

<p>另一個學習方法為GloVe，首先會計算context和target一起出現在訓練資料的次數，Xij會等於Xji如果是用target的正負距離來選擇context，但是如果只看target出現在context後面，就會不相等。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/GlovVe.JPG" class="" width="500">

<p>GloVe的模型中為了避免Xij為0，因此會乘上一個權重f(Xij)，如果Xij為0則f(Xij)為0，當然這個函式也可以用來給予不同詞頻的詞有不同的權重處理。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/GlovVe_model.JPG" class="" width="500">

<p>情感分析(Sentiment Analysis/Classification)是NLP其中一種應用，主要用在判斷文本中是表達喜歡或是不喜歡(正或負)。比如說，輸入的X為評論的內容，輸出Y為評論的情感等級，像是從網路評論中判斷對於餐廳或是旅館的正負評論。其最大的困難在於情感分析缺乏大量的標記資料來學習，如果使用word embedding可以幫助在少量訓練資料的情況下學習。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/sentiment_classification.JPG" class="" width="500">

<p>在建立情感分析模型時，一樣使將每個詞透過embedding matrix選出該詞的embeddings向量，接著透過加總或是取平均值後，再使用softmax函式得到輸出Yhat。不過因為這樣的方法是沒有前後順序的，所以如果遇到明明有否定詞在前面，但是後面出現很多的正向詞good，這可能就會造成誤判斷把明明是一星的評論判成五星，因為出現很多次的good會把結果導到正向，這個時候就需要使用RNN來建立模型。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/sentiment_classification_model.JPG" class="" width="500">

<p>使用RNN一開始也是將每個詞透過embeddings matrix找到該詞的embeddings向量，接著將每個詞輸入進RNN，最後再接softmax判斷結果，這個就一開始提到的Many to One的RNN類型。因為RNN是會有順序性的，所以對於出現否定詞的句子能夠處理的比較好。因為word embedding可以從較大的訓練資料訓練出來，所以即使在情感分析的訓練集裡面缺少了某些詞，但這些詞有被word embedding訓練過，這樣在作情感分析時也可以得到較好的結果。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/sentiment_classification_rnn.JPG" class="" width="500">

<p>在word embeddings的訓練過程中，可能會造成帶有偏見的學習結果，比如說本來是希望學出Man之於Woman等於King之於Queen，但可能會學習出Man之於Programer對於Woman之於Home_keeper這種帶偏見，或是Father之於Doctor對於Mother之於Nurse這種錯誤的結果。因為機器學習現在已經越來越普及的被應用在許多不同的領域上，所以這種帶偏見的錯誤結果應該要被避免的。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/bias_problem.JPG" class="" width="500">

<p>解決這種錯誤的學習有不同的方法，以解決性別偏見為例來說明，第一先將girl-boy和mother-father來找到屬於性別的分界向量，這時把任一個詞和這個分界向量作內積，就可以找到這個詞會偏向哪個性別，並找出bias direction，要讓不能帶有性別偏差的詞向量去除bias。透過Neutralize將一些對於性別來說屬於中性的詞，將其投影到non bias的方向軸，最後為確保像是doctor這種應該屬於中性的詞，與帶不同性別的詞(像Girl或是Boy)距離要是相同的，會使用Equalize pairs的手法將帶性別的詞移動到以軸為中性對稱的位置，這樣就會讓兩者與doctor的距離或是相似度是相同的。在選擇哪些詞需要進行Neutralize，可以透過練一個分類器來分辨；在選擇要equalize的部份因為較少量，所以可以簡單透過人工挑選的方式完成。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/bias_problem_addressing.JPG" class="" width="500">

<p>參考資料:<br/><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/nlp-sequence-models">Deep Learning Course 5 - Sequece Models</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一頁"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一頁"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目錄
        </li>
        <li class="sidebar-nav-overview">
          本站概要
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Drake"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Drake</p>
  <div class="site-description" itemprop="description">Chase excellence, success will follow!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">53</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分類</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">38</span>
        <span class="site-state-item-name">標籤</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/qiubite31" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;qiubite31" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/yu-long-lin/" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;yu-long-lin&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Drake</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 強力驅動
  </div>

        




  <script src="https://www.gstatic.com/firebasejs/6.3.3/firebase-app.js"></script>
  <script src="https://www.gstatic.com/firebasejs/6.3.3/firebase-firestore.js"></script>
  <script>
    firebase.initializeApp({
      apiKey   : 'AIzaSyBailFMNgblDDESyjXCS-hLzrVQuP0sUx0',
      projectId: 'myblog-313416'
    });

    function getCount(doc, increaseCount) {
      // IncreaseCount will be false when not in article page
      return doc.get().then(d => {
        var count = 0;
        if (!d.exists) { // Has no data, initialize count
          if (increaseCount) {
            doc.set({
              count: 1
            });
            count = 1;
          }
        } else { // Has data
          count = d.data().count;
          if (increaseCount) {
            // If first view this article
            doc.set({ // Increase count
              count: count + 1
            });
            count++;
          }
        }

        return count;
      });
    }

    function appendCountTo(el) {
      return count => {
        el.innerText = count;
      }
    }
  </script>
  <script>
    (function() {
      var db = firebase.firestore();
      var articles = db.collection('articles');

      if (CONFIG.page.isPost) { // Is article page
        var title = document.querySelector('.post-title').innerText.trim();
        var doc = articles.doc(title);
        var increaseCount = CONFIG.hostname === location.hostname;
        if (localStorage.getItem(title)) {
          increaseCount = false;
        } else {
          // Mark as visited
          localStorage.setItem(title, true);
        }
        getCount(doc, increaseCount).then(appendCountTo(document.querySelector('.firestore-visitors-count')));
      } else if (CONFIG.page.isHome) { // Is index page
        var promises = [...document.querySelectorAll('.post-title')].map(element => {
          var title = element.innerText.trim();
          var doc = articles.doc(title);
          return getCount(doc);
        });
        Promise.all(promises).then(counts => {
          var metas = document.querySelectorAll('.firestore-visitors-count');
          counts.forEach((val, idx) => {
            appendCountTo(metas[idx])(val);
          });
        });
      }
    })();
  </script>




      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://dragonlogdown.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>


</body>
</html>
