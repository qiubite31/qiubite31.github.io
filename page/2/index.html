<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"qiubite31.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="Chase excellence, success will follow!">
<meta property="og:type" content="website">
<meta property="og:title" content="Drake&#39;s">
<meta property="og:url" content="https://qiubite31.github.io/page/2/index.html">
<meta property="og:site_name" content="Drake&#39;s">
<meta property="og:description" content="Chase excellence, success will follow!">
<meta property="og:locale" content="zh_TW">
<meta property="article:author" content="Drake">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://qiubite31.github.io/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-TW'
  };
</script>

  <title>Drake's</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-82135937-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-82135937-1');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切換導航欄">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Drake's</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首頁</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>標籤</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分類</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>歸檔</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜尋
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜尋..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2018/02/27/andrew-deep-learning-course5-week3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/27/andrew-deep-learning-course5-week3/" class="post-title-link" itemprop="url">Andrew深度學習課程五 - Sequence Model第3周筆記</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2018-02-27 00:00:00" itemprop="dateCreated datePublished" datetime="2018-02-27T00:00:00+08:00">2018-02-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:34:12" itemprop="dateModified" datetime="2021-05-11T23:34:12+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2018/02/27/andrew-deep-learning-course5-week3/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/02/27/andrew-deep-learning-course5-week3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在RNN的應用中，有一種是sequence to sequence模型，像是在語言翻譯問題上，要把長度為5的法文翻譯成長度為6的英文。首先先透過一個encoder將每個法文字詞輸入進RNN模型，再透過decoder逐一輸出英文字詞直到結尾，這樣的模型被證實只要使用足夠大量的法文和英文句子就可以完成。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/sequence_to_sequence.JPG" class="" width="500">

<p>另一個類似的應用是給一張照片，然後自動給予這張照片適當的標題，這時可以透過CNN來建立一個encoder，接著再透過RNN建立一個decoder來產生適當的標題。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/image_caption.JPG" class="" width="500">

<p>第一週有學到language model，會計算產生句子的機率。而在翻譯問題中的encoder部分和language model非常相似，只是在輸入不是向量0，而是一個encoder的網絡。翻譯問題其實很像是一個conditional language model，即給定輸入翻譯前的句子X之下，輸出不同翻譯結果的機率。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/conditional_language_model.JPG" class="" width="500">

<p>我們並不希望直接對這個分布輸出作隨機的取樣，因為取樣的結果容易很不穩定，有時取到好的翻譯結果，有時取到不好的翻譯結果。所以在使用這個模型的時候，需要使用一個演算法來計算出給定X輸出譯翻結果Y的最大條件機率。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/most_likely_translate.JPG" class="" width="500">

<p>一個方法是使用貪婪搜尋法(Greedy Search)，即先根據conditional language model來產生第一個詞，接著在後面的翻譯部份每一次再依據前個詞選取最大機率來產生下個詞。但我們實際上希望的是找到輸出翻譯結果中每個詞Yi最大的聯合機率(Joint Probability)，而不是每次都產生一個機率最大的詞，比如說法文翻譯到英文的問題中，如果前兩個字的翻譯結果開頭都是Jane is…，但是going這個字在英文上比起visiting還常見，所以用這個方法會造成第3個詞比較容易接going，但是這並不是一個最佳化的翻譯結果。但在英文可選的詞非常多，所以會使用approximate搜尋法來嘗試找到最大的條件機率。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/greedy_search.JPG" class="" width="500">

<p>Beam Search演算法的特性在於不會像之前一樣只找一個最大的機率，而是會存下前B個最大的機率值在記憶體裡面。比如說第1個詞已經找出是in, jane和september，再來會繼續再找出第1個詞是in的情況下，第2個是哪個詞。這時會計算出在給定輸入X且第一個詞是in之下，哪個詞的機率最大，即P(Y2|X,Y1)。再將機率乘上P(Y1|X)就可以得到給定X之下，前兩個詞的機率P(Y1,Y2|X)。然後存下前三個最大的可能性，這時可能會存下in september, jane is和jane visiting，再繼續往下找第3個詞。這時也就代表演算法認為september不會是第一個詞。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/beam_search_step2.JPG" class="" width="500">

<p>接著beam search再從前兩個詞的三組詞往下展開來，並找出前三個詞最大機率的三組保留，透過這個方法持續到接到結尾EOS並停止。要注意的是如果把B設定成1的話，就等同於前面介紹的Greedy Search。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/beam_search2.JPG" class="" width="500">

<p>有些不同的方法可以讓beam search可以得到更好的結果，其中一個是length normalization。beam search會持續連乘使得機率值變很小，這可以透過取log相加取代，因為logP(y|x)和P(y|x)兩者取最大值將會有一樣的結果。當句子很長時，可以透過改變目標函式的方法來解決這個問題。因為取log後為&lt;=1，所以當詞越多值也就會越小，一個方法是除上詞的數量來作正規化，這可以大幅的降低因為句子太長所造成的懲罰(Penalty)效果。另外為了要讓這個正規化效果更加的平滑，可以將Ty取α次方，比如說設定成0.7(1為完整的正規化，0為不作正規化)</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/length_normalization.JPG" class="" width="500">

<p>但B值到底該如何選擇呢？B值越大會可以留下更多的組合來找到更好的結果，但卻訓練的時間會更久，而且也會使用更多的記憶體。Andrew這邊建立在production系統B為10就可以了，但是在研究的系統上，可以取更大的B來計算不同的組合，多次作嘗試。</p>
<p>因為翻譯問題不是像是圖像辨識一樣有正確的答案可以用accuracy來衡量，翻譯結果可能可以有多個一樣好的結果，這個時候可以使用Bleu(Biligual Evaluation Understudy) score來作訓練結果的衡量。<br/>Precision會比對機器翻譯出來的結果和人翻譯的結果，看機器翻譯的每個詞，是否都有出現在不同的答案reference中。比如果機器翻譯出來有7個詞，其中the這個詞都出現在reference中，所以precision為7/7。但這樣的計算顯然不夠精確。Modified Precision則會給予每個詞不同權重，像是the在reference 1出現2次，在reference 2出現1次，因此會給它權重為2，那邊Modified Precision將會修正2/7。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/evaluation_machine_translation.JPG" class="" width="500">

<p>但是衡量並不會只看單一詞的結果，比如說使用兩個詞為一組(Bigram)來作計算的話，會先透過bigram來取出所有組合，並計算每個組合在機器翻譯結果出現次數，再來計算每個組合在所有的reference中有沒有出現(1或0)，再來則將reference的數量除上機器翻譯bigram出現次數來計算 bigram的precision。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/bleu_score_bigram.JPG" class="" width="500">

<p>所以在使用N-gram取組合時，即可以歸納出一個公式，如果機器翻譯出來的結果和其中一組reference相同時，modified precision就會等於1。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/bleu_score_unigram.JPG" class="" width="500">

<p>把多個n-grams分數綜合起來，可以計算出一個綜合的分數再乘上BP。因為當機器的翻譯結果過短時，會造成大多的字存在reference中，使得modified precision會偏高，此時會引入BP(Brevity Penalty)的概念，當機器翻譯的長度比起reference還長時，BP為1即不給予懲罰分數；反之給予一個懲罰分數。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/bleu_detail.JPG" class="" width="500">

<p>實際上人在進行翻譯時，並不會把整段都看完再翻譯，而是先看一小段翻完再往後看另一小段。而且當句子越長時，Bleu score會逐漸降低，這時如果使用Attention Model來幫助每一小段作翻譯的話，則可以解決這個問題。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/long_sequence_problem.JPG" class="" width="500">

<p>雖然attention model是用在比較長的句子，但這邊使用短句來作說明。這裡encoder使用bidirectional RNN，在decoder時透過另一個RNN來完成翻譯。在產生第1個詞時會透過α來表示要產生這個詞應該要注意的資訊量有多少。接著再產生第2個詞時，會產生一組新的attention weight來表示產生第兩個詞時該注意的資訊量，並加上前一個產生的詞。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/attention_model_intuition.JPG" class="" width="500">

<p>在建立attention model時，encoder使用的bidirectional RNN會貢獻兩個activation值，此值即為α權重。接著在decoder產生每個Y值時會輸入C，這個C即為α值的加總，用來表示要產生的Y值需要注意多少的資料量，而這個α會取softmax確保其加總為1。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/attention_model.JPG" class="" width="500">

<p>所以在產生Y時主要有兩個輸入，一個是上個hidden狀態S&lt; t-1 &gt;，一個是要注意的資訊量a &lt; t’ &gt;，透過建立這個小的神經網路來使用梯度下降作訓練。但attention model的缺點就是他的計算成本較高，複雜度為Tx * Ty。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/computing_attention.JPG" class="" width="500">

<p>seqence to sequence可以被使用在文字的正規化，把不同型態的文字表達轉成一個相同的表示方法。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/attention_example.JPG" class="" width="500">

<p>語音辨識(Speech Recognition)也是一種sequence to sequence的問題，輸入一段語音後期望可以輸出一段正確的辨識結果。傳統的語音辨識方法會使用音位(Phoneme)當作基本的單位來表示語音結果，但在end to end的深度學習方法，已經不再需要使用這種方法。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/speech_recognition_problem.JPG" class="" width="500">

<p>一個有效的方法是使用CTC cost，語音辨識因為每秒可能有多次的取樣，所以輸入的資料會很大，光是10秒的語音就可以變成上1000的輸入，但是輸出的辨識結果並不會有上千的輸出。CTC可以協助幫輸出的結果作適當的collapse，其會對重複的字作collapse像是重複的t, e和q來產生較短的結果。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/CTC_cost.JPG" class="" width="500">

<p>這裡以trigger word偵測系為例來說明語音辨識問題，trigger word常被使用在一些智慧管家的產品當中，像是Google Home或是Amazon Echo。在訓練過程中會輸入語音，並在出現trigger word時的目標label設定為1，其它部份為0。</p>
<img src="/2018/02/27/andrew-deep-learning-course5-week3/trigger_word.JPG" class="" width="500">

<p>參考資料:<br/><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/nlp-sequence-models">Deep Learning Course 5 - Sequece Models</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2018/02/18/andrew-deep-learning-course5-week2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/18/andrew-deep-learning-course5-week2/" class="post-title-link" itemprop="url">Andrew深度學習課程五 - Sequence Model第2周筆記</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2018-02-18 00:00:00" itemprop="dateCreated datePublished" datetime="2018-02-18T00:00:00+08:00">2018-02-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:31:20" itemprop="dateModified" datetime="2021-05-11T23:31:20+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2018/02/18/andrew-deep-learning-course5-week2/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/02/18/andrew-deep-learning-course5-week2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在上一週的課程中，我們使用了one-hot結合詞庫來將句子作量化處理。但是使用one-hot的缺點是每個詞被視為獨立的，也就是無法衡量出兩個詞之間的相似或與重要性(兩個one-hot vector內積都會為0)。所以當你想知道apple和orange之間的關係比起apple和king還接近，就無法單純的使用one-hot來作文字的量化。其中一個方法是可以用不同的特性來作量化，比如說使用性別、是否為食物等等的特性，這個時候便可以辯識出具有相同性質的詞。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/word_embeddings_feature.JPG" class="" width="500">

<p>這種量化文字的方法又可以被稱為Word Embedding，像是每個詞會嵌在高維度的特徵向量中，即每個詞都可以在高維度的特徵找到一個能表示它的位置，而如果要將多個維度的特徵壓縮到兩維來表示，可以使用t-SNE演算法來找到相同的詞會彼此較相近。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/word_embeddings_visualize.JPG" class="" width="500">

<p>使用Word Embedding的好處除了可以判斷出兩個詞之間的相關係，也對Transfer learning有很大的幫助，即使用已經訓練好的word embeddig來用在新的任務。在使用word embedding於transfer learning有幾種方法。1. 從大量的詞庫(1-3B)中進行訓練(或是下載已被訓練過的) 2. 使用原本已建立的embedding並transfer到新的而且訓練資料集較小(1-3k)的任務 3. 持續的使用新的資料來對embeddings進行finetune。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/word_embeddings_transfer.JPG" class="" width="500">

<p>Word Embeddings也可以作到比擬(analogy)，比如說Man之於Woman等同於King之於Queen，這可以透過向量相減來找到相似的結果。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/analogy.JPG" class="" width="500">

<p>實際上要基於Man與Woman的關係來找到King和某個詞，可以透過向量相似度的計算來找到，只要找到King和哪一個詞的相似度與Man和Woman的相似度最接近即可。這裡的相似度計算是從原始的高維度(~300D)來的，雖然可以透過t-SNE來將高維度使用非線性的轉換到2維空間作視覺化，但如果使用了轉換的2維來計算反而會失真，所以在實際上的相似度計算會使用原始的高維度向量空間。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/analogy_vector.JPG" class="" width="500">

<p>一個常用的相似度計算是Cosine similarity，即計算兩個向量的角度來判斷兩個相向之間的相似度。Cosine similarity也經常被用在文本分析中，透過計算代表兩筆資料的高維度向量，來評估兩筆資料的相似度。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/cosine_similarity.JPG" class="" width="500">

<p>Word Embedding的學習最終會得到一個Embedding Matrix，凡是把這個矩陣乘上某一個詞的one-hot向量，所得到的即是代表這個詞的embedding。因為one-hot是高維度而且幾乎都是0的稀疏矩陣，所以在實務上並不會真的相乘，而且透過特別的方法來找到詞的embeddings。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/embedding_matrix.JPG" class="" width="500">

<p>在word embeddings的學習演算法中，過去往往會使用很多很複雜的演算法來學習，但後來卻發現很多簡單的方法就可以達到很好的效果。在複雜的方法中，比如說要預測一句話的下一個詞，可以把每個詞的word embeddings輸入到一個類神經網路學習，最後訓練出我們要的embedding matrix(E)。如果有300維度，那麼要輸入到網路裡面的維度就會多達300乘6共1800維，也可以選擇只看前4個詞來學習，那麼維度就會降到300乘4共1200維。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/other_context_target.JPG" class="" width="500">

<p>不過事實上用簡單一點的模型也可以訓練出好的效果，比如說只用預測詞的前一個字，或是最接近預測詞前一個詞的詞(又稱為skip gram)來訓練word embeddings matrix。</p>
<p>這裡介紹Word2Vec的Skip-grams模型，Skip-grams在選擇Context與Target組合時，針對Target可透過隨機挑選的方式選出不同的組合，比如說Context詞的+-5~10個詞來選出Target詞。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/skip_grams.JPG" class="" width="500">

<p>再來將這些對應的組合透過matrix E轉成word embeddings後丟入神經路網學習，接著經過softmax後計算loss function來訓練出matrix E。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/skip_grams_model.JPG" class="" width="500">

<p>但這個方法的主要缺點在於需要大量的計算，因為在計算softmax時，分別需要加總所有詞，如果詞的數量越多，那麼速度就會越慢，因此很難擴充到大型的訓練詞集。一個解法是使用hierarchical softmax，先拆出詞位於哪個部份(前或是後五千詞)，接著再接續往下作二元拆解。在tree的結構可以把常出現的通用詞放在上半部，這樣就可以較快的被找到。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/softmax_problem.JPG" class="" width="500">

<p>因為skip-grams在計算softmax需要花費大量運算，使用Negative Sampling則可以避免這個問題。Negative Sampling首先會先建立postive pair和negaive pair，比如說orange和juice會一起出現，就給予label 1，然後再隨機選出其他K個詞和orange組成negative pair給予label 0(即便某些時候negative pair組合真的有一起出現也沒關係)。最後將這些組合透過監督式學習訓練模型。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/negative_sampling.JPG" class="" width="500">

<p>接著可以使用logistic regression來計算在給定label為1時，postive pair組合的機率，並使用K+1組資料進行訓練，然後再將這個分類器應用在10000個詞作二元分類。因此比起原本作skip-grams要一次訓練10000組資料，使用negatiev sampling的方法只需要用少量的資料訓練，再一次用在多組詞的分類。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/negative_sampling_model.JPG" class="" width="500">

<p>但是該如何去選擇negative sampling呢？一個方法可以從詞出現的頻率選起，但是很容易選到停用詞(stop word)；或是假定他是均勻分配，使用1/|V|來選擇，而在這篇論文裡面是用了每個詞在訓練詞庫中出現的3/4次方來計算機率作選擇。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/negative_sampling_selection.JPG" class="" width="500">

<p>另一個學習方法為GloVe，首先會計算context和target一起出現在訓練資料的次數，Xij會等於Xji如果是用target的正負距離來選擇context，但是如果只看target出現在context後面，就會不相等。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/GlovVe.JPG" class="" width="500">

<p>GloVe的模型中為了避免Xij為0，因此會乘上一個權重f(Xij)，如果Xij為0則f(Xij)為0，當然這個函式也可以用來給予不同詞頻的詞有不同的權重處理。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/GlovVe_model.JPG" class="" width="500">

<p>情感分析(Sentiment Analysis/Classification)是NLP其中一種應用，主要用在判斷文本中是表達喜歡或是不喜歡(正或負)。比如說，輸入的X為評論的內容，輸出Y為評論的情感等級，像是從網路評論中判斷對於餐廳或是旅館的正負評論。其最大的困難在於情感分析缺乏大量的標記資料來學習，如果使用word embedding可以幫助在少量訓練資料的情況下學習。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/sentiment_classification.JPG" class="" width="500">

<p>在建立情感分析模型時，一樣使將每個詞透過embedding matrix選出該詞的embeddings向量，接著透過加總或是取平均值後，再使用softmax函式得到輸出Yhat。不過因為這樣的方法是沒有前後順序的，所以如果遇到明明有否定詞在前面，但是後面出現很多的正向詞good，這可能就會造成誤判斷把明明是一星的評論判成五星，因為出現很多次的good會把結果導到正向，這個時候就需要使用RNN來建立模型。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/sentiment_classification_model.JPG" class="" width="500">

<p>使用RNN一開始也是將每個詞透過embeddings matrix找到該詞的embeddings向量，接著將每個詞輸入進RNN，最後再接softmax判斷結果，這個就一開始提到的Many to One的RNN類型。因為RNN是會有順序性的，所以對於出現否定詞的句子能夠處理的比較好。因為word embedding可以從較大的訓練資料訓練出來，所以即使在情感分析的訓練集裡面缺少了某些詞，但這些詞有被word embedding訓練過，這樣在作情感分析時也可以得到較好的結果。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/sentiment_classification_rnn.JPG" class="" width="500">

<p>在word embeddings的訓練過程中，可能會造成帶有偏見的學習結果，比如說本來是希望學出Man之於Woman等於King之於Queen，但可能會學習出Man之於Programer對於Woman之於Home_keeper這種帶偏見，或是Father之於Doctor對於Mother之於Nurse這種錯誤的結果。因為機器學習現在已經越來越普及的被應用在許多不同的領域上，所以這種帶偏見的錯誤結果應該要被避免的。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/bias_problem.JPG" class="" width="500">

<p>解決這種錯誤的學習有不同的方法，以解決性別偏見為例來說明，第一先將girl-boy和mother-father來找到屬於性別的分界向量，這時把任一個詞和這個分界向量作內積，就可以找到這個詞會偏向哪個性別，並找出bias direction，要讓不能帶有性別偏差的詞向量去除bias。透過Neutralize將一些對於性別來說屬於中性的詞，將其投影到non bias的方向軸，最後為確保像是doctor這種應該屬於中性的詞，與帶不同性別的詞(像Girl或是Boy)距離要是相同的，會使用Equalize pairs的手法將帶性別的詞移動到以軸為中性對稱的位置，這樣就會讓兩者與doctor的距離或是相似度是相同的。在選擇哪些詞需要進行Neutralize，可以透過練一個分類器來分辨；在選擇要equalize的部份因為較少量，所以可以簡單透過人工挑選的方式完成。</p>
<img src="/2018/02/18/andrew-deep-learning-course5-week2/bias_problem_addressing.JPG" class="" width="500">

<p>參考資料:<br/><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/nlp-sequence-models">Deep Learning Course 5 - Sequece Models</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2018/02/05/andrew-deep-learning-course5-week1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/05/andrew-deep-learning-course5-week1/" class="post-title-link" itemprop="url">Andrew深度學習課程五 - Sequence Model第1周筆記</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2018-02-05 00:00:00" itemprop="dateCreated datePublished" datetime="2018-02-05T00:00:00+08:00">2018-02-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:30:11" itemprop="dateModified" datetime="2021-05-11T23:30:11+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2018/02/05/andrew-deep-learning-course5-week1/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/02/05/andrew-deep-learning-course5-week1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>有序列性質的資料(sequence data)可以泛指輸入資料(X)是有序列的，或是輸出資料(Y)是有序列的。比如說在翻譯問題上，輸入和輸出都是有序列性的句子；但在情感分析問題上，輸入會是一個有序列性質的評論句子，但是輸出為情感的等級或是分數。</p>
<img src="/2018/02/05/andrew-deep-learning-course5-week1/sequence_model_example.JPG" class="" width="500">

<p>量化句子的方法可以使用詞庫加上one-hot encoding，可以透過自己建立或是使用已存在的詞庫，並將詞庫使用one-hot encoding作編碼來將文字資料作量化產生可以用來學習的輸入資料X。</p>
<img src="/2018/02/05/andrew-deep-learning-course5-week1/Representing_words.JPG" class="" width="500">

<p>如果使用一般標準的類神經網路會有什麼問題呢？1. 每一個訓練資料的輸入和輸出的長度不同，不好處理。2. 在不同文字中的不同位置無法學習出共同的特徵。如果要像CNN一樣能夠在不同的訓練資料的不同位置中學習到共同的特徵，就必須要透過RNN來達成。</p>
<img src="/2018/02/05/andrew-deep-learning-course5-week1/recurrent_neural_networks.JPG" class="" width="500">

<p>Recurrent Neural Network(RNN)在每個訓練樣本會共同相同的weight和activation function。RNN會將每一個字詞X&lt; i &gt;經過激活函式後的結果，再傳給下一個字詞X&lt; i+1 &gt;，所以每個字詞會拿到由前面傳遞過來的資訊。但是只拿到前面字詞的資訊是不夠的，例如在判斷Teddy時，就需要使用字詞後面的句子內容，才能夠有效的區分Teddy究竟是指總統還是玩具。這時會使用到雙向的RNN(BRNN)才能夠解決這個問題。</p>
<img src="/2018/02/05/andrew-deep-learning-course5-week1/why_not_standard_network.JPG" class="" width="500">

<p>前面有提到RNN的輸入和輸出長度可能會不一樣長，針對不同輸入輸出長度可分為不同類型的RNN。在翻釋問題上，不同的語言可能翻譯後的長度都不同，這是一個Many to Many問題；如果是情感分析會將一則評論輸出成正反或是1到5星等級，屬於Many to One問題；One to Many會適用在像是音樂生成，給定一種音樂類型來產生一首音樂；One to One這種類型就是一般的神經網路了。</p>
<img src="/2018/02/05/andrew-deep-learning-course5-week1/RNN_type.JPG" class="" width="500">

<p>RNN可以被應用在建立語言模型(Language Model)，例如在語音辨識中，透過語言模型可以透過計算不同輸出句子的機率，來辨識出哪一個句子是最符合的輸出。</p>
<img src="/2018/02/05/andrew-deep-learning-course5-week1/language_model.JPG" class="" width="500">

<p>把每個句字斷詞並量化後，就可以丟進RNN來建立模型。舉例來說，在訓練時，將第一層RNN輸入空向量，再將本來的實際詞Y&lt; i &gt;從第兩次RNN依序丟進去訓練，就可以建立模型來預測某段句子後會接什麼樣的詞。即輸入Cats average，判斷下個接每個詞的機率。所以可以透過將每個詞依序輸入來計算出在給定某個句子下，輸出某個詞的機率。在這裡可以透過外部詞典，並使用softmax來算出每個詞的機率</p>
<img src="/2018/02/05/andrew-deep-learning-course5-week1/language_model_rnn.JPG" class="" width="500">

<p>進一步我們就可以使用這個模型來隨機的產生句子，首先先隨機選出一個詞，接著將每個詞依序丟進下個結點來產生新的詞，直到取出字尾EOS或是設定一個固定的句子長度終止。</p>
<img src="/2018/02/05/andrew-deep-learning-course5-week1/sample_sentense_word_level.JPG" class="" width="500">

<p>除了使用詞(Word-level)來組成句子，也可以使用字元(Character-level)來建且模型組成句子。如果使用字元來組成句子，那麼每一個輸出就是一個字元而非一個詞，好處在於不用怕取到辭典裡面沒有的詞，壞處則會產生更長的句子，也會降低訓練速度，所以目前經常是使用詞來建立模型。</p>
<h1 id="梯度消失-Gradient-Vanishing"><a href="#梯度消失-Gradient-Vanishing" class="headerlink" title="梯度消失(Gradient Vanishing)"></a>梯度消失(Gradient Vanishing)</h1><p>如果句子太長的話，使得神經網路過於深，會讓梯度在作back propagation時很難影響到前面的layer，產生梯度消失現象。對於RNN來說，實際上的影響在於如果太前面的神經元，將會無法去記住前面的詞(例如區分單數詞或是複數詞)。因此，傳統的RNN的輸出結果主要會受到接近神經元的影響，較遠的神經元不容易影響到最終的輸出。比起梯度爆炸容易被發現與處理，梯度消失反而不容易被處理，而且更容易影響到RNN的訓練結果。</p>
<p>Gate Recurrent Unit(GRU)是一個改善RNN長期記憶問題的方法，他引入了memory cell(C)的概念，在這裡的C就等同於activation function的輸出a，這個memory cell會將長期的記憶儲存下來，並用一個帶使用sigmoid轉換後的Γu值來判斷要不要選擇忘記之前的記憶並更新，還是要保留之前的記憶，直到不需時再忘記。實際上為什麼應用GRU會改善梯度消失的現象，主是要在Γu值透過sigmoid轉換後如數數值很小那麼就可能會非常的接近0，在這個情況下，C&lt; t &gt;就會非常接近C&lt; t-1 &gt;並很容易的保存較遠的記憶來大幅改善梯度消失的現象。</p>
<img src="/2018/02/05/andrew-deep-learning-course5-week1/GRU.JPG" class="" width="500">

<p>另一個常用的方法為Long Short Term Memory(LSTM)，在LSTM裡面C不再等同於a，LSTM會用到3種gate，分別為update gate, forget gate和output gate。</p>
<img src="/2018/02/05/andrew-deep-learning-course5-week1/GRU_and_LSTM.JPG" class="" width="500">

<p>透過update gate和forget gate來決定是否保存較長遠的記憶，並透過output gate來輸出a到下一個神經元。</p>
<img src="/2018/02/05/andrew-deep-learning-course5-week1/LSTM.JPG" class="" width="500">

<p>如果要同時合併前後神經元的資訊，就得使用Bidrectional RNN(BRNN)。BRNN除了從左到右作forward propagation外，也會從右到左作forward propagation。在輸出資料時，會同時輸入兩個方向進入activation function。而其中每個神經元也可以是GRU或是LSTM，所以實務上也會使用LSTM結合BRNN來建立模型。</p>
<img src="/2018/02/05/andrew-deep-learning-course5-week1/BRNN.JPG" class="" width="500">

<p>RNN可以疊多層起來變成深度的RNN模型，本來RNN每一個順序都只有連接一個神經元，但也可以在每一個序順接處多個神經元來建立深度的RNN模型，甚至結合GRU和LSTM與BRNN來建立複雜的RNN模型。不過因為深度的RNN在訓練是非常耗資源的，所以不太常見到超過3層的RNN模型。</p>
<img src="/2018/02/05/andrew-deep-learning-course5-week1/Deep_RNN.JPG" class="" width="500">

<p>參考資料:<br/><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/nlp-sequence-models">Deep Learning Course 5 - Sequece Models</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2017/12/05/keras-intro/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/12/05/keras-intro/" class="post-title-link" itemprop="url">第一次使用Keras就上手</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2017-12-05 00:00:00" itemprop="dateCreated datePublished" datetime="2017-12-05T00:00:00+08:00">2017-12-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:29:29" itemprop="dateModified" datetime="2021-05-11T23:29:29+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/12/05/keras-intro/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/12/05/keras-intro/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>最近正在學習Andrea在Coursera上開的Deep Learning課程，之前工作關係有接觸到keras，而且使用起來還滿容易上手的，所以就嘗試拿了MNIST資料集來試玩看看</p>
<p>首先載入手寫辨識資料集mnist，這個資料集還滿廣泛被拿來使用的，而且在keras也可以直接載入，另外也會用到最基本的keras Sequential model。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets </span><br><span class="line"><span class="keyword">import</span> mnistfrom keras.models </span><br><span class="line"><span class="keyword">import</span> Sequentialfrom keras.layers </span><br><span class="line"><span class="keyword">import</span> Denseimport numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br></pre></td></tr></table></figure>
<p>再來直接宣告一個sequential模型，並載入訓練和測試資料集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_train = np.reshape(x_train, (x_train.shape[<span class="number">0</span>], -<span class="number">1</span>))/<span class="number">255</span></span><br><span class="line">x_test = np.reshape(x_test, (x_test.shape[<span class="number">0</span>], -<span class="number">1</span>))/<span class="number">255</span></span><br><span class="line"></span><br><span class="line">y_train = np.eye(<span class="number">10</span>)[y_train.reshape(-<span class="number">1</span>)]</span><br><span class="line">y_test = np.eye(<span class="number">10</span>)[y_test.reshape(-<span class="number">1</span>)]</span><br></pre></td></tr></table></figure>
<p>接著要先處理一下載入的資料，在x資料的部份要先將原本28×28的維度轉成1×784輸入，這裡可以使用numpy的reshape來處理，再來再將資料除上255作正規化。另外在label y資料集的部份則要作one-hot encoding，將每個標籤轉成長度為10的向量，並用0和1來表示屬於哪一個類別。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x_train.shape</span><br><span class="line">(<span class="number">60000</span>, <span class="number">784</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x_test.shape</span><br><span class="line">(<span class="number">60000</span>, <span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_train.shape</span><br><span class="line">(<span class="number">10000</span>, <span class="number">784</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_test.shape</span><br><span class="line">(<span class="number">10000</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>可以看到處理完後的資料維度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.add(Dense(units=<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_dim=<span class="number">28</span>*<span class="number">28</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>再來加入兩個layer，即只使用一個hidden layer和一個output layer，其中hidden layer有256顆神經元，output layer有10顆，並透過softmax輸出結果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,</span><br><span class="line">              optimizer=<span class="string">&#x27;Adam&#x27;</span>,              </span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>接著開始設定使用什麼loss function與最佳化的方法，還有要評估模型的指標</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(x_train, y_train, epochs=<span class="number">10</span>, batch_size=<span class="number">32</span>)</span><br></pre></td></tr></table></figure>
<p>接著就開始訓練，其中會設定訓練的週期與每一次的批數</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss_and_metrics = model.evaluate(x_train, y_train, batch_size=<span class="number">128</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(loss_and_metrics)</span><br><span class="line">loss=<span class="number">0.007</span>, acc=<span class="number">0.998</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss_and_metrics = model.evaluate(x_test, y_test, batch_size=<span class="number">128</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(loss_and_metrics)</span><br><span class="line">loss=<span class="number">0.08</span>, acc=<span class="number">0.979</span></span><br></pre></td></tr></table></figure>
<p>中間可以看到訓練的過程，在訓練完畢說可以透過evaluate來評估model在訓練資料集，還有測試資料集的正確率。</p>
<p>keras在建立模型非常方便使用，可以很容易的加入需要的hidden layer數，而且針對常使用的activation function, loss function和最佳化的方法都有支援，如果需要快速的建出模型來作應用非常的推薦。另外keras也有支援CNN還有RNN，下次會用別的資料來試試看囉！</p>
<p>參考資料:<br/><a target="_blank" rel="noopener" href="https://keras.io/getting-started/sequential-model-guide/">Keras Getting Start</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2017/11/01/Machine-Learning-Foundation-16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/11/01/Machine-Learning-Foundation-16/" class="post-title-link" itemprop="url">機器學習基石(Machine Learning Foundation)第十六講筆記</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2017-11-01 00:00:00" itemprop="dateCreated datePublished" datetime="2017-11-01T00:00:00+08:00">2017-11-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:15:11" itemprop="dateModified" datetime="2021-05-11T23:15:11+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/11/01/Machine-Learning-Foundation-16/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/11/01/Machine-Learning-Foundation-16/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>上一堂提到validation的手法，透過留下用來驗證的資料模擬測試過程，並透過validation結果來選擇該使用什麼樣的模型。這一堂會提到三個在作機器學習時的小技巧。</p>
<img src="/2017/11/01/Machine-Learning-Foundation-16/OccamRazor.JPG" class="" width="500">

<p>Occam’s Razor在機器學習裡面意議是指不要對資料有過多的解釋，就是越簡單的解釋越好。以上面兩張圖的資料來看，左邊的模型符合直覺判斷，是一種比較容易而且 簡單的解釋，那到底什麼樣的模型才叫簡單的模型，而為什麼簡單的模型就比較好呢？</p>
<img src="/2017/11/01/Machine-Learning-Foundation-16/SimpleModel.JPG" class="" width="500">

<p>簡單的hypothesis是指沒有過多的參數就是個簡單的hypothesis，而簡單的模型則是指模型包含了較少的hypothesis就是個簡單的模型，所以簡單在這裡就是指比較小的hypothesis和模型複雜度。而要得到簡單的解釋，除了一開始就使用簡單的模型之外，也可以在之前透過regularization來達成。</p>
<img src="/2017/11/01/Machine-Learning-Foundation-16/SimpleIsBetter.JPG" class="" width="500">

<p>那為什麼簡單的模型就比較好呢？如果今天使用簡單的模型就可以將資料分類正確，那某種程度上也就代表著資料背後的關聯性或是規律性是簡單的；相反的如果使用很複雜的模型，可能就無法知道資料背後的關聯性，因為不管是有關聯性的資料，或是雜訊很多的資料，都可以被複雜的模型分的開。所以如果使用簡單的模型來解釋資料，可以很直覺的看到資料間的顯著性，但是如果使用複雜的模型就辨別不出來，所以建議一開始推薦先使用線性模型。</p>
<img src="/2017/11/01/Machine-Learning-Foundation-16/SamlingBias.JPG" class="" width="500">

<p>第二個技巧會談到樣本的抽樣誤差，這裡用一個美國總統選舉的例子，來說明如果抽樣和要學習的結果不一致，並帶出抽樣誤差問題。如果在抽樣時就發生抽樣誤差，那麼在學習時就會產生偏差的結果，這就是為什麼前面課堂有說到訓練和測試的樣本資料要抽樣自相同的分配，訓練和測試的資料抽自相同的分配，才會得到預期中的學習效果，這就是我們VC中的重要假設。</p>
<img src="/2017/11/01/Machine-Learning-Foundation-16/DealingSampleBias.JPG" class="" width="500">

<p>這裡舉了一個實際上發生過的問題，如果訓練資料和驗證資料有有時間前後依序性(即一個人看過的電影順序)，而非隨機取樣的話，如果透過隨機取樣來建立訓練資料和驗證資料，那麼在學習和驗證中就會有問題。這時候為了讓測試和驗證可以盡可能的接近，例如訓練時可以把時間依序性較後面的權重調高，或是抽比較多時間依序較後面的資料來作驗證。</p>
<img src="/2017/11/01/Machine-Learning-Foundation-16/VisualDataSnopping.JPG" class="" width="500">

<p>再來第三個技巧則是談到之前說到偷看資料的問題，前面有說到如果偷看了資料，可能會把人腦學習到的，或是自己的偏差帶進機器學習裡面。</p>
<img src="/2017/11/01/Machine-Learning-Foundation-16/DataSnopping.JPG" class="" width="500">

<p>偷看資料其實比想像中更容易發生，不是只有用眼睛視覺化的偷看才叫偷看，而是你在處理資料的整個過程中，都算是間接的偷看了資料。如果使用這樣偷看過的資料，都會受到自己的主觀影響。假設今天有一組八年的交易資料，使用前六年當訓練，後兩年當測試。其中在將資料作放縮(Data Scaling)的資料處理過程中，如果不是將前六年作縮放，預測完再還原，而是直接將八年的資料都作放縮的話，就會得到紅色這條上升趨線。這樣將會得到一個太過於樂觀的學習結果，如果將這個結果用來實際投資可能會大大的失準。</p>
<img src="/2017/11/01/Machine-Learning-Foundation-16/DataSnopping2.JPG" class="" width="500">

<p>除了直接的視覺化偷看，或是使用統計分析間接的偷看，其實作在研究上也會發生。例如針對相同問題，不同的論文會都使用更好的模型來作的比以前好，這樣的過程就有點像你的論文間接的偷看了前面論文的結果，這樣就有點像某種程度的overfit了。正是所謂的如果你拷問資料過久了，他就會招拱一個好的hypothesis，但是這個hypothesis應該用測試資料可能效果不保證會好。</p>
<img src="/2017/11/01/Machine-Learning-Foundation-16/DealingDataSnopping.JPG" class="" width="500">

<p>但是完全不偷看其實很不容易，只能盡量的降低這中間的干擾，比如說小心的使用validation，或是把測試資料好好的先收好。所以要時時注意的是，記得要用專業知識來建立模型，而不是先偷看了資料來作決策。另外要時時存著懷疑每次作出來的結果，並懷疑這樣的分析結果是不是有受過汙染。</p>
<img src="/2017/11/01/Machine-Learning-Foundation-16/ThreeRelateField.JPG" class="" width="500">

<p>這堂課教到很多和三有關的東西，第一個是三個和機器學習相關的領域，Data Mining是希望在大量資料中找到找到有用或是重要的關聯，人工智慧是要讓機器作出有智慧的事情(像是自動駕駛)，機器學習可以說是實現人工智慧的方法，統計則是為了去對母體作出推論，所以統計方法也被大量的使用在機器學習上。</p>
<img src="/2017/11/01/Machine-Learning-Foundation-16/ThreeTheoreticalBound.JPG" class="" width="500">

<p>在機器學習背後理論的保證，如果只有一個hypothesis的情況下，Hoeffding可以情供測試驗證的保證，當有多個hypothesis的情況下，Multi-Bin Hoeffding可以提供在有限多個選擇下的保證，如果是無限多個選擇下，VC則是可以提供在無限多個hypothesis是供理論上的保證。</p>
<img src="/2017/11/01/Machine-Learning-Foundation-16/ThreeLinearModel.JPG" class="" width="500">

<p>在機器學習模型部份，PLA/pocket可以提供在線性可分下處理二元分類問題，在衡量上為讓0/1 err最小化，linear regression則是可以處理數值預測問題，在衡量上使用squared err最小化，logistic regression則可以處理軟性二元分類問題，在衡量上使用cross entropy最小化。</p>
<img src="/2017/11/01/Machine-Learning-Foundation-16/ThreeKeyTool.JPG" class="" width="500">

<p>另外還有學到三個重要的技巧，Feature Transform可以將簡單的線性模型轉成高維度的複雜模型，會得到較好的Ein但是也會付出較高的VC代價，Regularization則是相反，透過加上regularizer來讓VC代價變小，但是也會讓Ein變大，Validation則是在沒辦法拿到測試資料的情況下，留下一部份的資料當作驗證資料。</p>
<img src="/2017/11/01/Machine-Learning-Foundation-16/ThreeLearningPrinciple.JPG" class="" width="500">

<p>最後則是這堂課學到的三個注意的地方，要注意簡單模型是好的，而且要注意抽樣的偏差，最後要記得不能提看資料。</p>
<img src="/2017/11/01/Machine-Learning-Foundation-16/ThreeFutureDirection.JPG" class="" width="500">

<p>再來後面的課程還會上到如何使用不同的轉換方法，以即不同的規則化方法，或是在缺少label的情況下該如何進行訓練。</p>
<p>總結這堂課程學到了很多機器學習背後的論理依據，而許多不同的機器學習方法將在另一堂機器學習技法課程教授！</p>
<p>參考資料:<br/><a target="_blank" rel="noopener" href="http://www.csie.ntu.edu.tw/~htlin/mooc/doc/16_handout.pdf">Machine Learning Foundation 16</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2017/10/25/Machine-Learning-Foundation-15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/10/25/Machine-Learning-Foundation-15/" class="post-title-link" itemprop="url">機器學習基石(Machine Learning Foundation)第十五講筆記</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2017-10-25 00:00:00" itemprop="dateCreated datePublished" datetime="2017-10-25T00:00:00+08:00">2017-10-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:15:11" itemprop="dateModified" datetime="2021-05-11T23:15:11+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/10/25/Machine-Learning-Foundation-15/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/10/25/Machine-Learning-Foundation-15/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>上一堂課講到為了避免overfitting，可以使用regularization的技巧，把一個regularizer加在Ein上面，轉而求Augmented Error反而可以有效的解決模型複雜度太高的問題。這一堂課會講到該如何使用Validation的手法幫助選擇機器學習裡面不同的參數。</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/ManyModel.JPG" class="" width="500">

<p>在訓練一個機器學習模型，其中會遇到很多的選擇，包含使用哪些演算法、使用多少資料、要使用什麼非線性轉換方法，甚至是要使用哪一種regularizer。</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/ModelSelection.JPG" class="" width="500">

<p>Machine Learning裡面最實務的問題，就是如何作出好的選擇讓最後Eout可以作到最好，如果用視覺化的方法來作選擇，反而會受到自己的主觀影響，那到底該怎麼作選擇呢？</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/ModelSelectionByEin.JPG" class="" width="500">

<p>如果使用Ein來作模型選擇的話，就會受到overfitting的影響，越複雜的模型就越容易作好更好的Ein，但是會付出模型複雜度的代價，因為overfitting造成泛化能力變差，Eout反而也作不好。</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/ModelSelectionByEtest.JPG" class="" width="500">

<p>那如果我們可以拿到實際的測試資料，就拿實際的測試資料衡量模型表現就好了，但是實務上可能不會有這樣的實際測試資料。</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/EinAndEtest.JPG" class="" width="500">

<p>如果使用Ein來選模型不可行，而實務上又拿不到實際的資料該怎麼取平衡呢？我們可以使用手上的樣本資料，把一部份切出來當作看不到的測試資料，再拿剩下的資料作學習，並使用切出來的資料衡量模型的效果。</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/ValidationSet.JPG" class="" width="500">

<p>原本的樣本資料D要負擔兩種角色，一個是透過D來求出最好的Ein，然後再把D丟進演算法得到一個Hypothesis G，前面有講到如果這樣的話很可能會造成overfitting問題，所以我們把資料分成train和validation，先使用train總共N-K筆資料練習出好的Ein，並得到Hypothesis G，最後再透過K筆validation資料來驗證G到底好還不好。</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/ModelSelectionByEval.JPG" class="" width="500">

<p>從前面講到的learning curve可以知道，如果使用較多的資料來訓練會得到比較好的結果，所以實務上雖然會使用D(train)來訓練出不同的Hypothesis，然後使用D(val)作評估並選出最好的模型，但是最後在選好模型之後，會再把全部的資料D丟進模型裡作訓練，因為使用全部的資料會比較用部份訓練資料所訓練出來的模型還要好。</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/ValidaionPractice.JPG" class="" width="500">

<p>從驗證資料量和Eout來分析可以看到黑線的Eout會很大，因為在使用複雜的模型之下可能因為overfitting的關係造成Eout作不好；而虛線是用Etest資料，但是這個最佳的測試資料常常是不存在的；紅色是只使用訓練資料建立模型得到的Eout，他會比藍色使用全部資料來訓練所得到的Eout還要來的高。至於為什麼使用訓練資料會比使用Ein還要來的差的原因，是因為當驗證資料量K越大，代表訓練資料就越小，可能產生underfitting使得模型怎麼樣都練習不好。</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/Dilemma.JPG" class="" width="500">

<p>所以我們會遇到不知道該怎麼選擇驗證資料量K的情況，因為如果用了比較大的K，雖然可以確保validation和Eout可以比較接近，但卻會造成訓練資料的Eout和全部資料的Eout差很多；但如果使用小的K雖然可以確保訓練資料的Eout和全部資料的Eout很接近，但是就無法確保validation和Eout到底接不接近。實務上的K常使用N/5。</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/K=1.JPG" class="" width="500">

<p>假設今天設定K=1，並且重複抽出一筆當驗證算出error，再來把error作平均，就可以透過計算這個平均的error來推估Eout。而這種只抽一筆當驗證的方法就稱為leave-one-out的cross validation，因為每次都用N-K筆計算一個g，所以稱為交叉驗證。</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/LOO.JPG" class="" width="500">

<p>在使用leave-one-out cross validation可以有兩種方法，第一種是拿掉一個點，用剩下的點作迴歸，再計算驗證的點到迴歸的距離當作error；第二種是拿掉一個貫，用剩下的點中間取一個常數，再計算驗證的點到常數的距離當作error，透過比較這兩種方法得到的較小的error，就可以選出比較好的模型，也許使用計算迴歸不一定會得到比較小的error，反而使用常數來計算還得到較好的結果。</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/GuaranteeOfLOO.JPG" class="" width="500">

<p>而且leave-one-out cross validation的期望值，確實也可以證明和Eout是相近的，這個部份證明這裡就不多作說明了。</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/LOOPractice.JPG" class="" width="500">

<p>在實務上，如果使用Ein來選擇模型，使用越多的特徵雖然可以得到較小的Ein，但卻也可以造成overfitting，反而和Eout相差很大。但是如果使用leave-one-out cross validation的方法來選模型，確實可以找到一個比較接近Eout的結果，而且絕對對比Ein要來的好很多。</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/DisadventageLOO.JPG" class="" width="500">

<p>使用leave-one-out的最大問題，就是他會花很多的時間在作訓練，如果樣本資料有1000筆，每次都要拿999筆來訓練並重複作一千次才能取到平均的Error，所以在實務上leave-one-out要用來選擇模型可能比較不可行。第二個問題是leave-one-out因為每次只取一個點，所以他的變動程度很高，雖然有取平均，但是他本質上還是一個變動程度比較高的評估方法。</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/VFoldCrossValidation.JPG" class="" width="500">

<p>所以實務上，第一個希望可以降低訓練的次數，比起之前的1000次訓練，可以切成10份的方式，每次取9份作訓練1份作驗證後作平均，這樣就只需要作10次而不需要作到1000次。而且這個方法也可以作到交叉驗證的效果，leave-one-out其實也就是這種方法的極端例子。實務上比較常用的方法是切成十份，又可以稱為10-fold cross validation</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/FinalWord.JPG" class="" width="500">

<p>使用cross validation在實務上會比只作一次validation當果會比較好也比較穩定，但是前提是在計算上是充許的，因為作k次的交叉驗證作平均需要額外的運算資源；而validation切5份或是10份就可以得到比較好的結果，不需要真的使用到leave-one-out。我們透過validation是可以讓我們選到一個比較好的模型，但是要注意的是，這個validation還是比最終使用測試資料的結果應該還是要樂觀的，只要真正應用在測試資料，你才會知道真的的效果如何。</p>
<img src="/2017/10/25/Machine-Learning-Foundation-15/Summary.JPG" class="" width="500">

<p>總結來說，這堂課學到使用Ein來選擇模型是很危險的，並談到leave-one-out cross validaion與實務上會使用到的k-fold corss validation可以用來作模型選擇。</p>
<p>參考資料:<br/><a target="_blank" rel="noopener" href="http://www.csie.ntu.edu.tw/~htlin/mooc/doc/15_handout.pdf">Machine Learning Foundation 15</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2017/10/18/Machine-Learning-Foundation-14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/10/18/Machine-Learning-Foundation-14/" class="post-title-link" itemprop="url">機器學習基石(Machine Learning Foundation)第十四講筆記</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2017-10-18 00:00:00" itemprop="dateCreated datePublished" datetime="2017-10-18T00:00:00+08:00">2017-10-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:15:11" itemprop="dateModified" datetime="2021-05-11T23:15:11+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/10/18/Machine-Learning-Foundation-14/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/10/18/Machine-Learning-Foundation-14/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>上一堂課講到overfitting現象，他會在使用過高的模型複雜度、雜訊過多或是資料太少時發生。上次有提到可以使用Data Clean/Purning和Data Hinting從資料面下手解決，這堂課會提到regularized手法來避免overfitting。</p>
<img src="/2017/10/18/Machine-Learning-Foundation-14/Regularization.JPG" class="" width="500">

<p>Overfit現象就是Target和Fit兩者相差太多，雖然可以將樣本學習的很好使Ein很小，但是卻造成泛化能力很差Eout過高，就如同右邊這張圖。regularized目標則是希望可以把Fit曲線能夠更接近Target，所以regularized可以視成一個從高次方項走向底次方項的手法，有很多的解都可以通過樣本點，但是究竟哪一組是最好的，就是regularized要作的。那該怎麼從高次多項走回低次多項式呢？</p>
<img src="/2017/10/18/Machine-Learning-Foundation-14/RegressionWithConstraint.JPG" class="" width="500">

<p>要從高次多項式走回低次多項式，其實就可以視成低次多項式是高次多項式加上constrain，即如果把十次多項式中超過三次的項權重都設成0，那麼就可以降回二次多項式了。在求最佳化的過程也是，如果今天要求二次多項式的最小值Ein，就可以使用加上constrain的十次多項式來求最小值。</p>
<img src="/2017/10/18/Machine-Learning-Foundation-14/RegressionWithLooserConstraint.JPG" class="" width="500">

<p>進一步的可以再延伸出，如果我並不是把超過三次項的權重設成0，而是只要其中有8項設權重設成0，就可以放寬這個constrain，讓regression可以更加的有彈性。這個放寬限制條件的二項式會比本來的二項式可以變化的項次更多，但也不像十次多項式這麼複雜。所以成求Ein時，只要確保權重為0的項次小於3個就可以了，但是這種離散的限制就像PLA一樣不容易求解，是NP-hard問題。</p>
<img src="/2017/10/18/Machine-Learning-Foundation-14/RegressionWithSofterConstraint.JPG" class="" width="500">

<p>原本的條件是算權重不是0的項次要小於3，我們可以把他轉換成將每個項次權重平均相加要小於上限制C，這樣就可以轉成比較好解的問題，而且權重越接近0的取平均會越小。C如果設定的越大，就會越接近十次多項式結果，且所有比較小的C的項次組合都會被比較大的C的項次組合包含。這個H(C)又可以被稱為regularized hypothesis，即加上限制條件的hypothesis，而透過規則化的hypothesis set裡所找到的最佳hypothesis又稱為W(REG)</p>
<img src="/2017/10/18/Machine-Learning-Foundation-14/TheLagrangeMultiplier.JPG" class="" width="500">

<p>再來可以和之前一樣把式子轉換成矩陣的表示方法來解最佳化問題。原本我們可以透過走梯度反方向到W(lin)這個最佳解，但是現在加上了限制式，這個限制在幾何上是一個圓圈，找到前面所提到的規則化hpothesis最佳解W(REG)的話，就得同時往梯度反方向走而且不離開圓邊。又因為往圓的法向量(紅色向量)走就會離開圓邊，所以只能往垂直於法向量的分量走(綠色向量)，當持續走到梯度反方向要是和圓的法向量平行的話，就代表不能走了，即為找到最佳解。我們可以把兩個平行向量的比值設成2λ/N。</p>
<img src="/2017/10/18/Machine-Learning-Foundation-14/AugmentedErr.JPG" class="" width="500">

<p>再來可以把求梯度等於0轉換回求最小值的式子，即為Ein加上一組regularizer又可稱為augmented error Eaug(w)，如果預先先指定λ值就可以很容易的解這個式子。</p>
<img src="/2017/10/18/Machine-Learning-Foundation-14/Result.JPG" class="" width="500">

<p>所以只要加上一點regularizer就可以對模型結果作出適當調整，今天如果λ設成0的話就等同沒有regularized即為overfitting，λ設的太大則會造成underfitting，加上regularizer就會有類似懲罰的效果。這種的規則化方法又稱為weight-decay regularization，即把權重最小的規則化方法。而且這個規則化方法可以應用到linear regresion、logistic regression甚至是其他不同的transform。</p>
<img src="/2017/10/18/Machine-Learning-Foundation-14/RegularizationAndVC.JPG" class="" width="500">

<p>但究竟這個augmented error和之前學到的VC有什麼相關性呢？用求augmented error的方法來解本來不好解的constrain Ein，這裡對應到的VC保證為Eout會小於En加上一個constrain H(C)，所以作好Eaug就能把Eout間接的也作好。</p>
<img src="/2017/10/18/Machine-Learning-Foundation-14/AnotherView.JPG" class="" width="500">

<p>Augmented error和VC bound其實相同的地方在於都是在求複雜度，augmented error求的是單一個hypothesis的複雜度，而VC則是在求整個hypothesis集合的複雜度。這個augmented error如果可以表現更好，那麼Eaug可能是一個比Ein更好的代理人可以幫我們作到好的Eout。</p>
<img src="/2017/10/18/Machine-Learning-Foundation-14/EffectiveVCDimension.JPG" class="" width="500">

<p>其實使用regulization付出的VC dimension會比原本還要小，因為在實際上有N個特徵維度下，最終透過regulization將不會使用到那麼多的維度。這裡相比原本的d(VC)，可以稱為d(EFF)，其付出的維度會比本來的d(VC)還小。</p>
<img src="/2017/10/18/Machine-Learning-Foundation-14/GeneralRegularizers.JPG" class="" width="500">

<p>那到底該使用什麼樣的regularizer呢？今天如果在知道target特性的情況下，可以針對target特性來設計regularizer，比如說如果想要一個比較接近偶函數的函數話，就針對奇數次方讓他變小。又或者在選出一個比較能夠說服我們的regularizer，像是比較平滑或是簡單的regularizer，因為overfitting是noise造成的，noise就會造成不平滑，像是使用L1 regularizer。或者也可以找一個好使用好最佳化的regularizer，像是前面說到的weight-decay regularizer，又被稱為L2 regularizer。如果找到不好的regularizer，那就把λ設定成0就等同於拿掉regularizer的效果。</p>
<img src="/2017/10/18/Machine-Learning-Foundation-14/L2AndL1.JPG" class="" width="500">

<p>L2 Regularizer最大的好處就是他很好微分求最佳值，而L1在微分求最佳值的部份比較困難，且L1在求最佳值時很常發生在頂點上，意義就是某一些的權重w會是0，所以L1 Regularizer又被稱為sparse regularizer。透過L1就可以在高維度空間下(例如1000維)，找到一些w非0的項次(可能只有5維)，所以在最後的預測只要算非0項速度會比較快。</p>
<img src="/2017/10/18/Machine-Learning-Foundation-14/OptimalLambda.JPG" class="" width="500">

<p>那λ該怎麼選擇呢？很明顯的可以看到不管是stochastic noise或是deterministic noise，只要noise越大λ就要越大。下一講將會開始說明在使用規則化手法時，該如何有效的使用最佳的λ。</p>
<img src="/2017/10/18/Machine-Learning-Foundation-14/Summary.JPG" class="" width="500">

<p>總結來看，這堂課說明規則化就是在原本的hypothesis加上一個條件，並轉成一個Augmented Error，因為作了規則化所以有些維度就不會被使用到，因此VC維度就會下降成d(EFF)，而使用regularizer的使用方法可以針對target特性，或是使用容易說服自己的regularizer，還有也可以使用好最佳化的regularizer。</p>
<p>參考資料:<br/><a target="_blank" rel="noopener" href="http://www.csie.ntu.edu.tw/~htlin/mooc/doc/14_handout.pdf">Machine Learning Foundation 14</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2017/10/13/Machine-Learning-Foundation-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/10/13/Machine-Learning-Foundation-13/" class="post-title-link" itemprop="url">機器學習基石(Machine Learning Foundation)第十三講筆記</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2017-10-13 00:00:00" itemprop="dateCreated datePublished" datetime="2017-10-13T00:00:00+08:00">2017-10-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:16:38" itemprop="dateModified" datetime="2021-05-11T23:16:38+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/10/13/Machine-Learning-Foundation-13/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/10/13/Machine-Learning-Foundation-13/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>上一講提到可以使用非線性轉換的方法，將線性的模型轉換成非線性，雖然可以解決更複雜的問題，但也伴隨著模型複雜度提高的代價。這一講將提到過度適合(Overfitting)現象所造成的泛化能力缺陷。</p>
<img src="/2017/10/13/Machine-Learning-Foundation-13/BadGeneralization.JPG" class="" width="500">

<p>假設今天有一個目標的函式是二次函式，但我們使用的四次函式來找到通過所有點的答案使得Ein為0，但這個四次函式卻和目標的二次函式差距很大造成Eout很大。這樣就會讓這個四次函式只認得樣本資料，而這種VC維度很高所付出的代價就是模型會失去舉一反三的能力，也就是泛化能力很差。</p>
<img src="/2017/10/13/Machine-Learning-Foundation-13/Overfitting.JPG" class="" width="500">

<p>這裡帶出學習會出現的兩個問題，一個是過度適合(Overfitting)，即Ein的fitting作的很好，但是作過度了造成Eout效果不好；一個是低度適合(Underfitting)，即Ein的fitting作的不夠好。Underfitting可以透過增加樣本資料或是使用較複雜的模型就可以解決，但是Overfitting反而是一個比較難解決的問題。</p>
<img src="/2017/10/13/Machine-Learning-Foundation-13/CauseofOverfitting.JPG" class="" width="500">

<p>發生Overfitting會有幾種原因，第一種是使用了過多的VC維，也就是使用了過於複雜的機器學習模型，像是對二次函式問題使用了四次函式來解；第二種可能性是雜訊太多，造成錯誤的學習；第三種是資料量不足，如果資料量不足可能沒有辦法學習出接近目標函式的結果。尤其當資料量不夠且雜訊又多，就很容易造成Overfitting讓模型最終失去泛化能力</p>
<img src="/2017/10/13/Machine-Learning-Foundation-13/CaseStudy.JPG" class="" width="500">

<p>假設現在有兩個目標函式，一個是十次多項式加上雜訊，一個是五十次多項式但是沒有雜訊。今天分別使用二次多項式和十次多項式來比較兩個問題的學習結果，會發現十次多項式都發生Overfitting，在Ein都作的比較好，但是Eout都作不好。甚至是當我們已經知道目標函式是十次多項式的情況，使用十次多項式的結果也不一定會贏過二次多項式。</p>
<img src="/2017/10/13/Machine-Learning-Foundation-13/LearningCurve.JPG" class="" width="500">

<p>用學習曲線來看會發現，當樣本數量不夠的時候，十次多項式Ein和Eout差距會非常大，所以如果資料量不足就不應該免強使用較複雜的方法，反而使用二次多項式還能夠學習出比較好的結果。</p>
<img src="/2017/10/13/Machine-Learning-Foundation-13/NoNoise.JPG" class="" width="500">

<p>那如果都沒有雜訊的情況下也會是二次多項式的效果比較好，因為當目標函式很複雜時，也會造成類似雜訊的效果，如果二次多項式和十次多項式都作不好，使用二次多項式的泛化效果可能會比較好。</p>
<img src="/2017/10/13/Machine-Learning-Foundation-13/DetailExperiment.JPG" class="" width="500">

<p>什麼時候要注意overfitting會發生呢？可以分成模型複雜程度Qf和雜訊程度σ^2來探討，再來會討論兩者和資料量N之間的關係。<img src="/2017/10/13/Machine-Learning-Foundation-13/TheOverfitMeasure.JPG" class="" width="500">延續之前的二次和十次多項式的例子，再來會使用Eout(g10) - Eout(g2)來衡量overfit的程度，即使用十次和二次多項式的Eout差來衡量。</p>
<img src="/2017/10/13/Machine-Learning-Foundation-13/Result.JPG" class="" width="500">

<p>在固定模型複雜度之下，只要資料量不夠而且雜訊程度越高，就越容易造成overfit；但在固定雜訊程度看模型複雜度的影響，大部份也是發生在資料量少且目標複雜度高會造成overfit。</p>
<img src="/2017/10/13/Machine-Learning-Foundation-13/NoiseAndSize.JPG" class="" width="500">

<p>兩者最主要的差異在於，雜訊是stochastic noise，即隨機產生的，但是目標函式的複雜度是deterministic noice，是固定可以計算出來的。總結來說在四種情況下會發生overfit，第一個是資料的量過小，第二個是stochastic noise太高的時候，第三個是deterministic noise太高的時候，第四個則是VC維度太高的時候。所以overfit是很容易發生的。</p>
<img src="/2017/10/13/Machine-Learning-Foundation-13/DeterministicNoise.JPG" class="" width="500">

<p>所以到底為什麼今天的目標函式太過複雜的情況下和隨機雜訊是類似的呢？假設今天目標函式太過複雜，以至於無法使用任何的hypothesis描述，這中間的差距就是我們說的deterministic noise，這並不是一個隨機發生的雜訊。所以deterministic noise會取決於hypothesis，而且在給定相同x之下會有相同的deterministic noise。</p>
<img src="/2017/10/13/Machine-Learning-Foundation-13/HandleOverfit.JPG" class="" width="500">

<p>如果有這麼多的原因會造成overfit，那該怎麼解決呢？在模型部份，可以先使用比較簡單的模型開始，避免一開始就用過複雜的模型；在資料部份，可以將雜訊資料作data cleaning/pruning處理，或是收集更多的資料與使用data hinting產生虛擬資料來提供更多額外的資料；另外還可以透過regularization對模型產生懲罰效果，或是透過validation來隨時看學習的狀況。</p>
<img src="/2017/10/13/Machine-Learning-Foundation-13/DataClean.JPG" class="" width="500">

<p>在Data Cleaning/Pruning的部份，Data Clean是指把已經確定是錯誤標記的資料，標記到正確的類別。Data Pruning則是直接將錯誤的雜訊資料直接從資料集去除掉。兩個資料的處理方法都不難，難度反而會在於該如何判斷資料是有問題的雜訊資料。</p>
<img src="/2017/10/13/Machine-Learning-Foundation-13/DataHinting.JPG" class="" width="500">

<p>在Data Hinging部份，以圖像的數字辨識來說，就是把每個字作簡單的旋轉來當作新的虛擬資料並加進來學習。這個方法很常用在現今在作圖象辨識時，對圖像作扭曲或是轉換來增加學習樣本數量。</p>
<img src="/2017/10/13/Machine-Learning-Foundation-13/Summary.JPG" class="" width="500">

<p>總結來說，這堂課教到了把Ein作好但是Eout作不好是overfitting現象，而且overfitting是非常容易發生的。不只是雜訊會造成overfitting，當目標函式過於複雜時，也是另一種雜訊。在處理overfitting的部份，這堂課簡單提到了data cleaning/pruning和data hinting，下一堂會再進一步教到如何使用regularization來避免overfitting現象。</p>
<p>參考資料:<br/><a target="_blank" rel="noopener" href="http://www.csie.ntu.edu.tw/~htlin/mooc/doc/13_handout.pdf">Machine Learning Foundation 13</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2017/10/02/Machine-Learning-Foundation-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/10/02/Machine-Learning-Foundation-12/" class="post-title-link" itemprop="url">機器學習基石(Machine Learning Foundation)第十二講筆記</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2017-10-02 00:00:00" itemprop="dateCreated datePublished" datetime="2017-10-02T00:00:00+08:00">2017-10-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:15:11" itemprop="dateModified" datetime="2021-05-11T23:15:11+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/10/02/Machine-Learning-Foundation-12/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/10/02/Machine-Learning-Foundation-12/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>上一堂講到三種不同的線性模型都可以用在二元分類，並且還可以透過二元分類來達成多類別分類，這一堂課會教到該如何將線性模型延伸轉換成非線性模型。</p>
<img src="/2017/10/02/Machine-Learning-Foundation-12/LinearHypotheses.JPG" class="" width="500">

<p>前面都講到線性模型，線性模型可以很容易的計算出線性分數，再結合不同的轉換作輸出，線性模型好處在於VC維度是可以很容易的受到控制的，所以確保作好Ein，Eout就可以表現的好。但是如果遇到像右邊比較複雜的資料，可能就沒辦法使用線性模型達成，勢必需要結合其它手法來突破限制。</p>
<img src="/2017/10/02/Machine-Learning-Foundation-12/CircularSeparable.JPG" class="" width="500">

<p>再重新看一次這組比較複雜的資料，可以發現他雖然沒辦法線性分割，但似乎是「圓圈」可分的問題，也就是在圓圈內和圓圈外是不同的類別。那該如何把本來線性可分的問題轉成圓圈可分，再透過前面教到的演算法來解決呢？</p>
<img src="/2017/10/02/Machine-Learning-Foundation-12/CircularSeparable2.JPG" class="" width="500">

<p>接著我們先把原本的圓圈作符號的替換，可以整理成像之前一樣的向量內積式。他的意義在於，如果本來可以這個問題在X維度上用圓圈分出兩種類別，那麼在Z維度上將可以用直接用直線來分出兩種類別，即線性可分問題，這樣的轉換又稱為特徵轉換。所以我們知道問題在X維是圓圈可分，轉成在Z維是線性可分，但是相反來說的是不是在Z維是線性可分，在X維就是圓圈可分呢？</p>
<img src="/2017/10/02/Machine-Learning-Foundation-12/LinearHypothesesZSpace.JPG" class="" width="500">

<p>在Z維是線性可分，但是在X維其實不一定會是圓圈。因為在二次曲線可能性不止是圓圈，還有可能是橢圓或是雙曲線，甚至就算是圓圈還可能分成圓內和圓外，那該如何找到所有二次曲線的可能性呢？</p>
<img src="/2017/10/02/Machine-Learning-Foundation-12/GeneralQuadraticHypothesisSet.JPG" class="" width="500">

<p>我們可以列出所有能表式二次曲線的項都列出來(包含常數、一次項和二次項)，所以在Z維的perceptron就會對應到在X維的某個二次曲線可能的分類方式，當然也有可能會退化成一次方程式。</p>
<img src="/2017/10/02/Machine-Learning-Foundation-12/GoodQuadraticHypothesis.JPG" class="" width="500">

<p>所以到現在可以發現，如果能夠在Z維找到一個好的perceptron，就可以在X維找到一個可分割類別的二次曲線。之前學習的內容都是在X維裡面使用x和y來找到好的perceptron，現在要在Z維找到好的perceptron，自然就需要使用在Z維上的資料，再使用學習到的二元分類方法來處理。</p>
<img src="/2017/10/02/Machine-Learning-Foundation-12/NonlinearModel.JPG" class="" width="500">

<p>首先可以透過一個函式Φ來將X維資料轉成Z維，再使用演算法來找出一個線性分割的perceptron。於是我們就可以把X維的資料，把它丟到Z維看看轉換到Z維的這筆資料究竟是什麼類別，就可以知道這筆資料該分成什麼類別。所以當我們把原本學到的方法透過特徵轉換到二次式甚至是多項式，可大大延伸演算法的應用範圍！</p>
<img src="/2017/10/02/Machine-Learning-Foundation-12/FeatureTransform.JPG" class="" width="500">

<p>比如說本來維度很高的手寫辨識資料，可以應用特徵轉換建立intensity和symmetry組成的兩維問題。</p>
<img src="/2017/10/02/Machine-Learning-Foundation-12/ComputationStoragePrice.JPG" class="" width="500">

<p>這樣的特徵轉換好看起很多好處，但是究竟會有什麼壞處呢？前面只講到兩次項而已，如果今天要把特徵轉成Q次項的話，他的複雜度將會是O(Q^d)，其中d是除常數項以外的項次，Q是要作的Q次項轉換。所以可以發現，透過這個轉換會讓計算還有儲存都多花費額外的力氣，可能是很困難的。</p>
<img src="/2017/10/02/Machine-Learning-Foundation-12/ModelComplexityPrice.JPG" class="" width="500">

<p>除了計算和儲存的困難，在VC維也會因為Q變大而變大，造成模型的複雜度變高。</p>
<img src="/2017/10/02/Machine-Learning-Foundation-12/GeneralizationIssue.JPG" class="" width="500">

<p>上面給出了一組資料且有兩種分類結果，雖然右邊的Ein為0(即所有資料都可以分對)，但是相比起左邊的分類結果反而讓人比較喜歡。雖然左邊的分類結果有些點是分錯的Ein不是0，但是線性的模型的泛化能力會比較好，雖然特徵轉換的維度比較低，Ein比較大，但是確有可能得到一個比較好的Eout；右邊的模型其特徵轉換的維度比較高，雖然讓Ein可以作的很好，但有可能得到較差的Eout，這就是機器學習上特徵還有模型選擇的trade-off！</p>
<img src="/2017/10/02/Machine-Learning-Foundation-12/VisualChoices.JPG" class="" width="500">

<p>那到底該選擇哪個維度的轉換才是最好的呢？也許就先把資料作視覺化看看資料長什麼樣子就能決定了，像是正圓圈只要作二次曲線轉換就可以了。看似好像一下子就可以找到特徵轉換的維度來達到好的學習效果，但是這樣的過程其實包含了過多的人為介入，而這樣的介入也許可以得到不錯的學習結果與好的Ein，但是背後隱含了機器會受到你的主觀想法和偏見影響，也容易讓你對學習的結果有過度的樂觀。所以在機器學習中要非常的小心，也許我們可以先偷看資料，然後用腦袋解析一次來找到最好的維度轉換與好的Ein，但是這個學習的結果在拿來應用時卻可能會得到不好的效果。</p>
<img src="/2017/10/02/Machine-Learning-Foundation-12/PolynomialTransformRevisited.JPG" class="" width="500">

<p>在多維度的轉換過程中，當轉換成Q維時，其中也包含了Q-1維的項次，Q-1維能作到的事Q維也能作到，Q-1維就等同於在Q維中某些項次為0，這其實是一個巢狀的涵蓋蓋念。</p>
<img src="/2017/10/02/Machine-Learning-Foundation-12/StructuredHypothesisSets.JPG" class="" width="500">

<p>維度越高，可調整的自由變數就越多，所以VC維也就越多；又因為可調整的自由變數越多，所以也就越有可能找到更好的結果來產生更好的Ein。再來就可以畫出之前也看過的圖，Ein會隨著VC維而下降，而模型複雜度則會隨著VC維而上升，其中我們最在乎的Eout會先下降再上升。所以如果一開始就使用很高維度的特徵轉換是會得到好的Ein，但也會付出模型複雜度的代價，造成Eout效果不好。</p>
<img src="/2017/10/02/Machine-Learning-Foundation-12/LinearModelFirs.JPG" class="" width="500">

<p>安全的作法是先從低維轉換開始，再持續往右邊移動，直接找到好的Ein與好的Eout。雖然線性分類也許可以作到的是有限的，但是線性的泛化能力比較好，反而最後可以作到的事情是比較多的。</p>
<img src="/2017/10/02/Machine-Learning-Foundation-12/Summary.JPG" class="" width="500">

<p>這一堂課教的是把原本線性的模型作特徵轉換變成非線性，並說明其中轉換的流程該如何作，但要特別注意的是維度的轉換是會付出計算、儲存和模型複雜度代價的，在選擇維度轉換記得要從簡單的開始，慢慢的往高維度找到最好的模型。</p>
<p>參考資料:<br/><a target="_blank" rel="noopener" href="http://www.csie.ntu.edu.tw/~htlin/mooc/doc/12_handout.pdf">Machine Learning Foundation 12</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://qiubite31.github.io/2017/09/28/Machine-Learning-Foundation-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Drake">
      <meta itemprop="description" content="Chase excellence, success will follow!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Drake's">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/09/28/Machine-Learning-Foundation-11/" class="post-title-link" itemprop="url">機器學習基石(Machine Learning Foundation)第十一講筆記</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              <time title="創建時間：2017-09-28 00:00:00" itemprop="dateCreated datePublished" datetime="2017-09-28T00:00:00+08:00">2017-09-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2021-05-11 23:15:11" itemprop="dateModified" datetime="2021-05-11T23:15:11+08:00">2021-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B%E7%AD%86%E8%A8%98/" itemprop="url" rel="index"><span itemprop="name">線上課程筆記</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="閱讀次數">
              <span class="post-meta-item-icon">
                <i class="fa fa-users"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span class="firestore-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/09/28/Machine-Learning-Foundation-11/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/09/28/Machine-Learning-Foundation-11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>上一講談到logistic regression，這一講會講到到底該如何使用線性的模型來作二元或是多元分類。</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/LinearModelsRevisited.JPG" class="" width="500">

<p>複習一下三種線性模型，共同點就是三種都會透過計算分數來作分類，差別在於PLA的線性分類會將分數取正負號來達到0/1分類，線性迴歸則是直接輸出分數，並使用squared error評估找到最佳解；logistic regression則是會透過logistic function將分數轉換成0到1的機率值。</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/ErrorFunctionsRevisited.JPG" class="" width="500">

<p>為了將線性模型作二元分類(y=-1/+1)，可以把error function稍微整理算出yx項，這個yx項的實際意義為分類的正確性，也就是yx項得到的分數要是正的(即兩者同號)，而且越大越好。</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/VisualizingErrorFunctions.JPG" class="" width="500">

<p>再來將三種模型的error畫出來，可以發現三種error的特性都不同，唯一相同地方在於如果squared和scaled cross-entropy很低時，通常0/1也會很低。</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/TheoreticalImplicationUpperBound.JPG" class="" width="500">

<p>究竟linear regression和logistic regression是否是好的分類方法呢？先從VC的角度來看scaled cross-entropy他會是0/1 error的upper bound，所以如果把logistic regression的cross-entropy error作到最小的話，也就可以說我們能把0/1 error也作的好。當然sqr err也是一樣，所以linear regression和logistic regression確實是可以作到二元分類的。</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/RegressionClassification.JPG" class="" width="500">

<p>在logistic/linear regression分類問題上，linear regression的好處是他最容易作到最佳化，但是其error衡量比較寬鬆；logistic regression因為他也是convex所以最佳化也是容易的，而error衡量在ys負向很小時會很寬鬆，但也比linear regression還好；PLA則是如果在線性可分的問題上可以作到很好，不過缺點就是如果在非性線可分的問題上，雖然可以使用pocket演算法，但是效果就沒那麼好。</p>
<p>以前有有學過，雖然linear regression太過寬鬆，但是卻可以很快的先拿在找到一個初始的權重值，後續再交給PLA或是logistic regression作後續的最佳化。在實務上大部份的人會較常使用logistic regression來作二元分類，因為可以兼顧效果還有最佳化的容易程度。</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/IterativeOptimization.JPG" class="" width="500">

<p>雖然PLA和logistic regression都是iterative optimization方法，但是PLA是每看一點就作調整，而logistic regression卻要看完所有的資料點才會一次調整權重，他的速度和pocket一樣，每作一輪都要花比較長的時間，到底能不能讓logistic regression和PLA一樣快呢？</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/StochasticGradientDescent.JPG" class="" width="500">

<p>那不就學PLA每看一個點就調整一次權重就好了啊，這種每看一點作偏微分求梯度來調整權重的方法就稱為Stochastic Gradient Desent(SGD)，即用隨機的梯度作梯度下降來接近真實梯度的期望值。好處就是每次算一個點比較簡單而且容易計算，由其是在大量資料的情況下原本的批次梯度下降法速度會很慢，但是壞就就是一次看一個點所以每次調整的結果可能很不穩定，但是最終應該都會很接近要求的目標值。這種隨機梯度下降也適合online learning，即每次接到一筆新資料後即作學習調整權重。其實還有另外折衷的方法稱為mini-batch gradient desent，當我們沒辦法看完所有資料再調整權重時，那就一次看N筆資料作調整就好囉！</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/PLARevisited.JPG" class="" width="500">

<p>那PLA和logistic regression到底相差在哪裡呢？PLA是一次調整到位，而logistic是看差多少就調多少，所以SGD的logistic可以說他像soft的PLA，而PLA又很像η設成1的logistic。在執行logistic時有兩個可以調整討論的地方，第一個是停止條件，通常我們會執行夠多的次數，因為我們相信執行夠多次就會逐步接近目標執；另外是η的設定上，老師個人習慣會使用0.1126，也許之後需要調整學習速度時可以參考一下。</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/MulticlassClassification.JPG" class="" width="500">

<p>再來談到多類別分類的問題，實務上常會有很多多類別的問題，像是要辨識圖像上不同的東西就是多類別的問題，再來會介紹該怎麼使用二元分類來達成多類別分類。</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/MulticlassPrediction.JPG" class="" width="500">

<p>當我們想分類其中一種類別時，可以把其他三種當成同一種類別，這樣就可以建立四種二元分類器，把多類別問題轉成二元分類問題。但是這樣的方法會有某些地方有分類重疊的問題，像是四個角落都有兩種分類器範圍重疊，甚至也會有某些地方所有分類器都分不出來，像是中間區塊所有分類器都會認為不是自己要分類出來的區域。那該怎麼辦呢？</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/OneClassSoftly.JPG" class="" width="500">

<p>我們可以應用logistic regression來達成softly classfication，用顏色深淺來代表機率的大小，顏色越藍分到O機率越大，反之顏色越紅分到X的機率大。</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/SoftClassifiers.JPG" class="" width="500">

<p>再來組合出四個分類器，就可以知道在給定一組資料X，他究竟有分到不同類別的的機率有多少，所以可以透過選出機率最大的類別來判斷要分成哪個類別。</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/OVA.JPG" class="" width="500">

<p>當我們有K種類別要作分類時，透過logistic regression建立K個分類器，接著選出機率最高的類別，這種方法來作多類別分類稱為One-Versus-All(OVA)。好處是方法很有效率，而且只要和logistic regression能算出機率值的方法，都可以應用OVA作多類別分類，壞處是如果成兩元分類的資料不平衡的話，可能造成每個分類器都叫你猜大宗類別，造成最終分類效果不好。</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/Unbalance.JPG" class="" width="500">

<p>前面有講到OVA會遇到Unbalance問題，可能造成分類表現不好，那該怎麼解決這個問題呢？</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/OneVersusOne.JPG" class="" width="500">

<p>前面是一對其他所有類別作兩元分類，我們其實可以直接使用其中兩種類別的資料作二元分類就好了，如果兩種類別的資料接近的話，那麼資料量就會比較平均避免Unbalance問題。</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/PairwiseClassifiers.JPG" class="" width="500">

<p>那有這麼多的分類器，到底該如何判斷是哪個類別呢？以方塊類別為例，其中可以發現共6個分類器，其中前三個都說分出來的結果是方塊，後面三個則分類分到一個菱形和兩個星型，透過投票可以知道最可能的類別應該會是方塊，因為分出方塊佔大多數。</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/OVO.JPG" class="" width="500">

<p>這樣的方法稱為One-versus-one(OVO)，因為他是一對一類別的分類，而非一對其他所有類別的分類方法，透過一對一類別的分類別，再找出最有可能的的類別。這個方法的優點是在訓練資料時很有效果，因為只使用比較少的資料量來作訓練，而且可以應用在所有的二元分類問題；壞處則是要訓練更多的分類器，所以在預測的時間、訓練的次數還有儲存的空間都會花比較多。</p>
<img src="/2017/09/28/Machine-Learning-Foundation-11/Summary.JPG" class="" width="500">

<p>這一講首先學到了前面講的三種線種模型方法其實都是可以用來作二元分類的，而且針對logistic regression可以使用SGD來作隨機梯度下降找最佳解，這樣的方法很像最早學到的PLA。再來針對多類別的問題教了兩種方法，都是應用二元分類來達成多類別分類，第一種OVA是把所有資料分成兩個類別，一個是要分出來的類別，另一個是把其他資料視為同一類別；第二種OAO是單純就看兩種不同類別來作兩兩比較，而這兩種多類別方法都是簡單而且常被拿來使用的分類手法！</p>
<p>參考資料:<br/><a target="_blank" rel="noopener" href="http://www.csie.ntu.edu.tw/~htlin/mooc/doc/11_handout.pdf">Machine Learning Foundation 11</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一頁"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一頁"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目錄
        </li>
        <li class="sidebar-nav-overview">
          本站概要
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Drake"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Drake</p>
  <div class="site-description" itemprop="description">Chase excellence, success will follow!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">45</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分類</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">標籤</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/qiubite31" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;qiubite31" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/yu-long-lin/" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;yu-long-lin&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Drake</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 強力驅動
  </div>

        




  <script src="https://www.gstatic.com/firebasejs/6.3.3/firebase-app.js"></script>
  <script src="https://www.gstatic.com/firebasejs/6.3.3/firebase-firestore.js"></script>
  <script>
    firebase.initializeApp({
      apiKey   : 'AIzaSyBailFMNgblDDESyjXCS-hLzrVQuP0sUx0',
      projectId: 'myblog-313416'
    });

    function getCount(doc, increaseCount) {
      // IncreaseCount will be false when not in article page
      return doc.get().then(d => {
        var count = 0;
        if (!d.exists) { // Has no data, initialize count
          if (increaseCount) {
            doc.set({
              count: 1
            });
            count = 1;
          }
        } else { // Has data
          count = d.data().count;
          if (increaseCount) {
            // If first view this article
            doc.set({ // Increase count
              count: count + 1
            });
            count++;
          }
        }

        return count;
      });
    }

    function appendCountTo(el) {
      return count => {
        el.innerText = count;
      }
    }
  </script>
  <script>
    (function() {
      var db = firebase.firestore();
      var articles = db.collection('articles');

      if (CONFIG.page.isPost) { // Is article page
        var title = document.querySelector('.post-title').innerText.trim();
        var doc = articles.doc(title);
        var increaseCount = CONFIG.hostname === location.hostname;
        if (localStorage.getItem(title)) {
          increaseCount = false;
        } else {
          // Mark as visited
          localStorage.setItem(title, true);
        }
        getCount(doc, increaseCount).then(appendCountTo(document.querySelector('.firestore-visitors-count')));
      } else if (CONFIG.page.isHome) { // Is index page
        var promises = [...document.querySelectorAll('.post-title')].map(element => {
          var title = element.innerText.trim();
          var doc = articles.doc(title);
          return getCount(doc);
        });
        Promise.all(promises).then(counts => {
          var metas = document.querySelectorAll('.firestore-visitors-count');
          counts.forEach((val, idx) => {
            appendCountTo(metas[idx])(val);
          });
        });
      }
    })();
  </script>




      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://dragonlogdown.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>


</body>
</html>
